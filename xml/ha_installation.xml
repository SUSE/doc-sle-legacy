<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
    type="text/xml"
    title="Profiling step"
?>
<!DOCTYPE chapter
[
   <!ENTITY % entities SYSTEM "entity-decl.ent">
   %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.1" xml:id="cha-ha-installation">
<?dbfo-need height="20em"?>
 <title>Installation and Basic Setup</title>
 <info>
  <abstract>
   <para>
    This chapter describes how to install and set up &productnamereg;
    &productnumber; from scratch. Choose between an automatic setup or a manual
    setup. The automatic setup enables you to have a cluster up and running
    within a few minutes (with the choice to adjust any options later on),
    whereas the manual setup allows you to set your individual options right at
    the beginning.
   </para>
<!--https://bugzilla.novell.com/show_bug.cgi?id=573817#c6-->
   <para>
    Refer to chapter <xref linkend="app-ha-migration"/> if you want to migrate
    a cluster that runs an older version of &productname;, or if you want to
    update software packages on nodes that belong to a running cluster.
   </para>
  </abstract>
 </info>
 <section xml:id="sec-ha-installation-terms">
  <title>Definition of Terms</title>
  <para>
   This chapter uses several terms that are defined below.
  </para>
  <variablelist>
   <varlistentry>
    <term>Existing Cluster</term>
    <listitem>
     <para>
      &def-existing-cluster;
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Multicast</term>
    <listitem>
     <para>
      &def-multicast; If multicast does not comply with your corporate IT
      policy, use unicast instead.
     </para>
     <note>
      <title>Switches and Multicast</title>
      <para>
       If you want to use multicast for cluster communication, make sure your
       switches support multicast.
      </para>
     </note>
    </listitem>
   </varlistentry>
   <varlistentry xml:id="vle-ha-mcastaddr">
    <term>Multicast Address (<systemitem>mcastaddr</systemitem>)</term>
    <listitem>
     <para>
      &def-mcastaddr; If IPv6 networking is used, node IDs must be specified.
      You can use any multicast address in your private network.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Multicast Port (<systemitem>mcastport</systemitem>)</term>
    <listitem>
     <para>
      &def-mcastport; &corosync; uses two ports: the specified
      <literal>mcastport</literal> for receiving multicast, and
      <literal>mcastport -1</literal> for sending multicast.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Unicast</term>
    <listitem>
     <para>
      &def-unicast;
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Bind Network Address (<systemitem>bindnetaddr</systemitem>)</term>
    <listitem>
     <para>
      &def-bindnetaddr; To ease sharing configuration files across the cluster,
      &ais; uses network interface netmask to mask only the address bits that
      are used for routing the network. For example, if the local interface is
      <literal>192.168.5.92</literal> with netmask
      <literal>255.255.255.0</literal>, set
      <systemitem>bindnetaddr</systemitem> to <literal>192.168.5.0</literal>.
      If the local interface is <literal>192.168.5.92</literal> with netmask
      <literal>255.255.255.192</literal>, set
      <systemitem>bindnetaddr</systemitem> to <literal>192.168.5.64</literal>.
     </para>
     <note>
      <title>Network Address for All Nodes</title>
      <para>
       As the same &corosync; configuration will be used on all nodes, make
       sure to use a network address as <systemitem>bindnetaddr</systemitem>,
       not the address of a specific network interface.
      </para>
     </note>
    </listitem>
   </varlistentry>
   <varlistentry xml:id="vle-ha-rrp">
    <term>Redundant Ring Protocol (RRP)</term>
    <listitem>
     <para>
      &def-rrp; A logical token-passing ring is imposed on all participating
      nodes to deliver messages in a reliable and sorted manner. A node is
      allowed to broadcast a message only if it holds the token. For more
      information, refer to
      <link xlink:href="http://corosync.github.io/corosync/doc/icdcs02.ps.gz"/>.
     </para>
     <para>
      When having defined redundant communication channels in &corosync;, use
      RRP to tell the cluster how to use these interfaces. RRP can have three
      modes (<literal>rrp_mode</literal>):
     </para>
     <itemizedlist mark="bullet" spacing="normal">
      <listitem>
       <para>
        If set to <literal>active</literal>, &corosync; uses both interfaces
        actively.
       </para>
      </listitem>
      <listitem>
       <para>
        If set to <literal>passive</literal>, &corosync; sends messages
        alternatively over the available networks.
       </para>
      </listitem>
      <listitem>
       <para>
        If set to <literal>none</literal>, RRP is disabled.
       </para>
      </listitem>
     </itemizedlist>
<!--With RRP, two physically separate networks are used for communication. In case one
      network fails, the cluster nodes can still communicate via the other network.-->
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>&csync;</term>
    <listitem>
     <para>
      &def-csync2; &csync; can handle any number of hosts, sorted into
      synchronization groups. Each synchronization group has its own list of
      member hosts and its include/exclude patterns that define which Ô¨Åles
      should be synchronized in the synchronization group. The groups, the host
      names belonging to each group, and the include/exclude rules for each
      group are specified in the &csync; configuration file,
      <filename>/etc/csync2/csync2.cfg</filename>.
     </para>
     <para>
      For authentication, &csync; uses the IP addresses and pre-shared keys
      within a synchronization group. You need to generate one key file for
      each synchronization group and copy it to all group members.
     </para>
     <para>
      For more information about &csync;, refer to
      <link xlink:href="http://oss.linbit.com/csync2/paper.pdf"/>
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><systemitem class="resource">conntrack</systemitem> Tools</term>
    <listitem>
     <para>
      &def-conntrack; For detailed information, refer to
      <link xlink:href="http://conntrack-tools.netfilter.org/"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>&ay;</term>
    <listitem>
     <para>
      &def-ay; On &sle; you can create an &ay; profile that contains
      installation and configuration data. The profile tells &ay; what to
      install and how to configure the installed system to get a ready-to-use
      system in the end. This profile can then be used for mass deployment in
      different ways (for example, to clone existing cluster nodes).
     </para>
     <para>
      For detailed instructions on how to use &ay; in various scenarios, see
      the <citetitle>&sle; &productnumber; &deploy;</citetitle>, available at
      <link xlink:href="http://www.suse.com/doc"/>. Refer to chapter
      <citetitle>Automated Installation</citetitle>.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </section>
 <section xml:id="sec-ha-installation-overview">
  <title>Overview</title>
  <para>
   The following basic steps are needed for installation and initial cluster
   setup.
<!--They can
   either be executed manually or automatically, using one the methods mentioned in <xref
    linkend="sec-ha-installation-methods"/>.-->
  </para>
  <procedure>
   <step>
    <para>
     <xref linkend="sec-ha-installation-add-on" xrefstyle="select:title"/>:
    </para>
    <para>
     Install the software packages with &yast;. Alternatively, you can install
     them from the command line with <command>zypper</command>:
    </para>
<screen>&prompt.root;<command>zypper</command> in -t pattern ha_sles</screen>
<!--SLE HA GEO: <screen>zypper in -t pattern ha_geo</screen>-->
   </step>
   <step>
    <para>
     Initial Cluster Setup:
    </para>
    <para>
     After installing the software on all nodes that will be part of your
     cluster, the following steps are needed to initially configure the
     cluster.
    </para>
    <substeps performance="required">
     <step>
      <para>
       <xref linkend="sec-ha-installation-setup-channels" xrefstyle="select:title"/>
      </para>
     </step>
     <step>
      <para>
       Optional:
       <xref linkend="sec-ha-installation-setup-security" xrefstyle="select:title"/>
      </para>
     </step>
     <step>
      <para>
       <xref linkend="sec-ha-installation-setup-csync2" xrefstyle="select:title"/>.
       Whereas the configuration of &csync; is done on one node only, the
       services &csync; and <systemitem class="daemon">xinetd</systemitem> need
       to be started on all nodes.
      </para>
     </step>
     <step>
      <para>
       Optional:
       <xref linkend="sec-ha-installation-setup-conntrackd" xrefstyle="select:title"/>
      </para>
     </step>
     <step>
      <para>
       <xref linkend="sec-ha-installation-setup-services" xrefstyle="select:title"/>
      </para>
     </step>
     <step>
      <para>
       <xref linkend="sec-ha-installation-start" xrefstyle="select:title"/>.
       The &ais;/&corosync; service needs to be started on all nodes.
      </para>
     </step>
    </substeps>
   </step>
  </procedure>
  <para>
   The cluster setup steps can either be executed automatically (with bootstrap
   scripts) or manually (with the &yast; cluster module or from command line).
  </para>
  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     If you decide for an automatic cluster setup, refer to
     <xref linkend="sec-ha-installation-setup-auto"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     For a manual setup (or for adjusting any options after the automatic
     setup), refer to <xref linkend="sec-ha-installation-setup-manual"/>.
    </para>
   </listitem>
  </itemizedlist>
  <para>
   You can also use a combination of both setup methods, for example: set up
   one node with &yast; cluster and then use <command>sleha-join</command> to
   integrate more nodes.
  </para>
  <para>
   Existing nodes can also be cloned for mass deployment with &ay;. The cloned
   nodes will have the same packages installed and the same system
   configuration. For details, refer to
   <xref linkend="sec-ha-installation-autoyast"/>.
  </para>
 </section>
 <section xml:id="sec-ha-installation-add-on">
  <title>Installation as Add-on</title>
  <para>
   The packages needed for configuring and managing a cluster with the &hasi;
   are included in the <literal>&ha;</literal> installation pattern. This
   pattern is only available after &productname; has been installed as an
   add-on to &slsreg;. For information on how to install add-on products, see
   the <citetitle>&sle; &productnumber; &deploy;</citetitle>, available at
   <link xlink:href="http://www.suse.com/doc"/>. Refer to chapter
   <citetitle>Installing Add-On Products</citetitle>.
<!--taroth: need to use hard-coded link here as the target is not included in the same set-->
  </para>
  <procedure xml:id="pro-ha-install-pattern">
   <title>Installing the &ha; Pattern</title>
   <step>
    <para>
     To install the packages via command line, use Zypper:
    </para>
<screen>sudo <command>zypper</command> in -t pattern ha_sles</screen>
   </step>
   <step>
    <para>
     Alternatively, start &yast; as &rootuser; user and select <menuchoice>
     <guimenu>Software</guimenu> <guimenu>Software Management</guimenu>
     </menuchoice>.
    </para>
    <para>
     It is also possible to start the &yast; module as &rootuser; on a command
     line with <command>yast2&nbsp;sw_single</command>.
    </para>
   </step>
   <step>
    <para>
     From the <guimenu>Filter</guimenu> list, select
     <guimenu>Patterns</guimenu> and activate the <guimenu>High
     Availability</guimenu> pattern in the pattern list.
    </para>
   </step>
   <step>
    <para>
     Click <guimenu>Accept</guimenu> to start installing the packages.
    </para>
    <note>
     <title>Installing Software Packages on All Parties</title>
     <para>
      The software packages needed for &ha; clusters are
      <emphasis>not</emphasis> automatically copied to the cluster nodes.
     </para>
    </note>
   </step>
   <step>
    <para>
     Install the &ha; pattern on <emphasis>all</emphasis> machines that will be
     part of your cluster.
    </para>
    <para>
     If you do not want to install &sls; &productnumber; and &productname;
     &productnumber; manually on all nodes that will be part of your cluster,
     use &ay; to clone existing nodes. For more information, refer to
     <xref linkend="sec-ha-installation-autoyast"/>.
    </para>
   </step>
  </procedure>
 </section>
 <section xml:id="sec-ha-installation-setup-auto">
  <title>Automatic Cluster Setup (sleha-bootstrap)</title>
  <para>
   The <systemitem class="resource">sleha-bootstrap</systemitem> package
   provides everything you need to get a one-node cluster up and running, to
   make other nodes join, and to remove nodes from an existing cluster:
  </para>
  <variablelist>
   <varlistentry>
    <term><xref linkend="pro-ha-installation-setup-sleha-init" xrefstyle="select:title"/></term>
    <listitem>
     <para>
      With <command>sleha-init</command>, define the basic parameters needed
      for cluster communication and (optionally) set up a &stonith; mechanism
      to protect your shared storage. This leaves you with a running one-node
      cluster.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><xref linkend="pro-ha-installation-setup-sleha-join" xrefstyle="select:title"/></term>
    <listitem>
     <para>
      With <command>sleha-join</command>, add more nodes to your cluster.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><xref linkend="pro-ha-installation-setup-sleha-remove" xrefstyle="select:title"/></term>
    <listitem>
     <para>
      With <command>sleha-remove</command>, remove nodes from your cluster.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
  <para>
   All commands execute bootstrap scripts that require only a minimum of time
   and manual intervention. The bootstrap scripts for initialization and
   joining automatically open the ports in the firewall that are needed for
   cluster communication. The configuration is written to
   <filename>/etc/sysconfig/SuSEfirewall2.d/services/cluster</filename>. Any
   options set during the bootstrap process can be modified later with the
   &yast; cluster module.
  </para>
  <para>
   Before starting the automatic setup, make sure that the following
   prerequisites are fulfilled on all nodes that will participate in the
   cluster:
  </para>
  <itemizedlist mark="bullet" spacing="normal">
   <title>Prerequisites</title>
   <listitem>
    <para>
     The requirements listed in <xref linkend="sec-ha-requirements-sw"/> and
     <xref linkend="sec-ha-requirements-other"/> are fulfilled.
    </para>
   </listitem>
   <listitem>
    <para>
     The <systemitem class="resource">sleha-bootstrap</systemitem> package is
     installed.
    </para>
   </listitem>
   <listitem>
    <para>
     The network is configured according to your needs. For example, a private
     network is available for cluster communication and network device bonding
     is configured. For information on bonding, refer to
     <xref linkend="cha-ha-netbonding"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     If you want to use SBD for your shared storage, you need one shared block
     device for SBD. The block device need not be formatted. For more
     information, refer to <xref linkend="cha-ha-storage-protect"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     All nodes must be able to see the shared storage via the same paths
     (<filename>/dev/disk/by-path/...</filename> or
     <filename>/dev/disk/by-id/...</filename>).
    </para>
   </listitem>
  </itemizedlist>
  <procedure xml:id="pro-ha-installation-setup-sleha-init">
   <title>Automatically Setting Up the First Node</title>
   <para>
    The <command>sleha-init</command> command checks for configuration of NTP
    and guides you through configuration of the cluster communication layer
    (&corosync;), and (optionally) through the configuration of SBD to protect
    your shared storage. Follow the steps below. For details, refer to the
    <command>sleha-init</command> man page.
   </para>
   <step>
    <para>
     Log in as &rootuser; to the physical or virtual machine you want to use as
     cluster node.
    </para>
   </step>
   <step>
    <para>
     Start the bootstrap script by executing
    </para>
<screen>&prompt.root;sleha-init</screen>
    <para>
     If NTP has not been configured to start at boot time, a message appears.
    </para>
    <para>
     If you decide to continue anyway, the script will automatically generate
     keys for SSH access and for the &csync; synchronization tool and start the
     services needed for both.
    </para>
   </step>
   <step>
    <para>
     To configure the cluster communication layer (&corosync;):
    </para>
    <substeps performance="required">
     <step>
      <para>
       Enter a network address to bind to. By default, the script will propose
       the network address of <systemitem>eth0</systemitem>. Alternatively,
       enter a different network address, for example the address of
       <literal>bond0</literal>.
      </para>
     </step>
     <step>
      <para>
       Enter a multicast address. The script proposes a random address that you
       can use as default.
      </para>
     </step>
     <step>
      <para>
       Enter a multicast port. The script proposes <literal>5405</literal> as
       default.
      </para>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     To configure SBD (optional), enter a persistent path to the partition of
     your block device that you want to use for SBD. The path must be
     consistent across all nodes in the cluster.
    </para>
    <para>
<!--FIXME: what happens in the background? any resources added?-->
    </para>
    <para>
     Finally, the script will start the &ais; service to bring the one-node
     cluster online and enable the Web management interface &hawk;. The URL to
     use for &hawk; is displayed on the screen.
    </para>
   </step>
   <step>
    <para>
     For any details of the setup process, check
     <filename>/var/log/sleha-bootstrap.log</filename>.
    </para>
   </step>
  </procedure>
  <para>
   You now have a running one-node cluster. Check the cluster status with
   <command>crm&nbsp;status</command>:
  </para>
<screen>&prompt.root;crm status
   Last updated: Thu Jul  3 11:04:10 2014
   Last change: Thu Jul  3 10:58:43 2014
   Current DC: alice (175704363) - partition with quorum
   1 Nodes configured
   0 Resources configured
      
   Online: [ alice ]</screen>
  <important>
   <title>Secure Password</title>
   <para>
    The bootstrap procedure creates a Linux user named
    <systemitem class="username">hacluster</systemitem> with the password
    <literal>linux</literal>. You need it for logging in to &hawk;. Replace the
    default password with a secure one as soon as possible:
   </para>
<screen>&prompt.root;<command>passwd</command> hacluster</screen>
  </important>
  <procedure xml:id="pro-ha-installation-setup-sleha-join">
   <title>Adding Nodes to an Existing Cluster</title>
   <para>
    If you have a cluster up and running (with one or more nodes), add more
    cluster nodes with the <command>sleha-join</command> bootstrap script. The
    script only needs access to an existing cluster node and will complete the
    basic setup on the current machine automatically. Follow the steps below.
    For details, refer to the <command>sleha-join</command> man page.
   </para>
   <para>
    If you have configured the existing cluster nodes with the &yast; cluster
    module, make sure the following prerequisites are fulfilled before you run
    <command>sleha-join</command>:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      The &rootuser; user on the existing nodes has SSH keys in place for
      passwordless login.
     </para>
    </listitem>
    <listitem>
     <para>
      &csync; is configured on the existing nodes. For details, refer to
      <xref linkend="pro-ha-installation-setup-csync2-yast"/>.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    If you are logged in to the first node via &hawk;, you can follow the
    changes in cluster status and view the resources being activated in the Web
    interface.
   </para>
   <step>
    <para>
     Log in as &rootuser; to the physical or virtual machine supposed to join
     the cluster.
    </para>
   </step>
   <step>
    <para>
     Start the bootstrap script by executing:
    </para>
<screen>&prompt.root;sleha-join</screen>
    <para>
     If NTP has not been configured to start at boot time, a message appears.
    </para>
   </step>
   <step>
    <para>
     If you decide to continue anyway, you will be prompted for the IP address
     of an existing node. Enter the IP address.
    </para>
   </step>
   <step>
    <para>
     If you have not already configured a passwordless SSH access between both
     machines, you will also be prompted for the &rootuser; password of the
     existing node.
    </para>
    <para>
     After logging in to the specified node, the script will copy the
     &corosync; configuration, configure SSH and &csync;, and will bring the
     current machine online as new cluster node. Apart from that, it will start
     the service needed for &hawk;.
<!--taroth 2011-11-08: maybe
      remove the following sentence? asked tserong-->
     If you have configured shared storage with OCFS2, it will also
     automatically create the mountpoint directory for the OCFS2 file system.
    </para>
   </step>
   <step>
    <para>
     Repeat the steps above for all machines you want to add to the cluster.
    </para>
   </step>
   <step>
    <para>
     For details of the process, check
     <filename>/var/log/sleha-bootstrap.log</filename>.
    </para>
   </step>
  </procedure>
  <para>
   Check the cluster status with <command>crm&nbsp;status</command>. If you
   have successfully added a second node, the output will be similar to the
   following:
  </para>
<screen>&prompt.root;crm status
   Last updated: Thu Jul  3 11:07:10 2014
   Last change: Thu Jul  3 10:58:43 2014
   Current DC: alice (175704363) - partition with quorum
   2 Nodes configured
   0 Resources configured
   
   Online: [ alice bob ]</screen>
  <important>
   <title>Check <systemitem>no-quorum-policy</systemitem></title>
   <para>
    After adding all nodes, check if you need to adjust the
    <systemitem>no-quorum-policy</systemitem> in the global cluster options.
    This is especially important for two-node clusters. For more information,
    refer to <xref linkend="sec-ha-config-basics-global-quorum"/>.
   </para>
  </important>
<!--taroth 2013-03-06: fate#313489-->
  <procedure xml:id="pro-ha-installation-setup-sleha-remove">
   <title>Removing Nodes From An Existing Cluster</title>
   <para>
    If you have a cluster up and running (with at least two nodes), you can
    remove single nodes from the cluster with the
    <command>sleha-remove</command> bootstrap script. You need to know the IP
    address or host name of the node you want to remove from the cluster.
    Follow the steps below. For details, refer to the
    <command>sleha-remove</command> man page.
   </para>
<!--taroth 2013-03-12: is it required that rcopenais is running one the node I
    want to take down? taroth 2013-03-12: according to tserong: no, does not
    matter-->
   <step>
    <para>
     Log in as &rootuser; to one of the cluster nodes.
    </para>
   </step>
   <step>
    <para>
     Start the bootstrap script by executing:
    </para>
<screen>&prompt.root;sleha-remove -c <replaceable>IP_ADDR_OR_HOSTNAME</replaceable></screen>
    <para>
     The script enables the <systemitem class="daemon">sshd</systemitem>, stops
     the &ais; service on the specified node, and propagates the files to
     synchronize with &csync; across the remaining nodes.
    </para>
<!--information taken from bnc#810292, c#5-->
    <para>
     If you specified a host name and the node to remove cannot be contacted
     (or the host name cannot be resolved), the script will inform you and ask
     whether to remove the node anyway. If you specified an IP address and the
     node cannot be contacted, you will be asked to enter the host name and to
     confirm whether to remove the node anyway.
    </para>
   </step>
   <step>
    <para>
     To remove more nodes, repeat the step above.
    </para>
   </step>
   <step>
    <para>
     For details of the process, check
     <filename>/var/log/sleha-bootstrap.log</filename>.
    </para>
   </step>
  </procedure>
  <para>
   If you need to re-add the removed node at a later point in time, add it with
   <command>sleha-join</command>. For details, refer to
   <xref linkend="pro-ha-installation-setup-sleha-join"/>.
  </para>
  <procedure>
   <title>Removing the &hasi; Software From a Machine</title>
   <para>
    To remove the &hasi; software from a machine that you no longer need as
    cluster node, proceed as follows.
   </para>
   <step>
    <para>
     Stop the cluster service:
    </para>
<screen>&prompt.root;rcopenais stop</screen>
   </step>
   <step>
    <para>
     Remove the &hasi; add-on:
    </para>
<screen>&prompt.root;<command>zypper</command> rm -t products sle-hae</screen>
   </step>
  </procedure>
 </section>
 <section xml:id="sec-ha-installation-setup-manual">
  <title>Manual Cluster Setup (&yast;)</title>
  <para>
   See <xref linkend="sec-ha-installation-overview"/> for an overview of all
   steps for initial setup.
  </para>
  <section xml:id="sec-ha-installation-setup-yast2cluster">
   <title>&yast; Cluster Module</title>
   <para>
    The following sections guide you through each of the setup steps, using the
    &yast; cluster module. To access it, start &yast; as &rootuser; and select
    <menuchoice> <guimenu>&ha;</guimenu> <guimenu>Cluster</guimenu>
    </menuchoice>. Alternatively, start the module from command line with
    <command>yast2&nbsp;cluster</command>.
   </para>
   <para>
    If you start the cluster module for the first time, it appears as wizard,
    guiding you through all the steps necessary for basic setup. Otherwise,
    click the categories on the left panel to access the configuration options
    for each step.
   </para>
   <figure>
    <title>&yast; Cluster Module&mdash;Overview</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="yast2_cluster_main.png" width="100%" format="PNG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="yast2_cluster_main.png" width="75%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
   <para>
    Note that some options in the &yast; cluster module apply only to the
    current node, whereas others may automatically be transferred to all nodes.
    Find detailed information about this in the following sections.
   </para>
  </section>
  <section xml:id="sec-ha-installation-setup-channels">
   <title>Defining the Communication Channels</title>
   <para>
    For successful communication between the cluster nodes, define at least one
    communication channel.
   </para>
   <important>
    <title>Redundant Communication Paths</title>
    <para>
     It is highly recommended to set up cluster communication via two or more
     redundant paths. This can be done via:
    </para>
    <itemizedlist mark="bullet" spacing="normal">
     <listitem>
      <para>
       <xref linkend="cha-ha-netbonding" xrefstyle="select:title"/>.
      </para>
     </listitem>
     <listitem>
      <para>
       A second communication channel in &corosync;. For details, see
       <xref linkend="pro-ha-installation-setup-channel2"/>.
      </para>
     </listitem>
    </itemizedlist>
    <para>
     If possible, choose network device bonding.
    </para>
   </important>
   <procedure xml:id="pro-ha-installation-setup-channel1">
    <title>Defining the First Communication Channel</title>
    <para>
     For communication between the cluster nodes, use either multicast (UDP) or
     unicast (UDPU).
    </para>
    <step>
     <para>
      In the &yast; cluster module, switch to the <guimenu>Communication
      Channels</guimenu> category.
     </para>
    </step>
    <step>
     <para>
      To use multicast:
     </para>
     <substeps performance="required">
      <step>
       <para>
        Set the <guimenu>Transport</guimenu> protocol to
        <literal>UDP</literal>.
       </para>
      </step>
      <step>
       <para>
        Define the <guimenu>Bind Network Address</guimenu>. Set the value to
        the subnet you will use for cluster multicast.
       </para>
      </step>
      <step>
       <para>
        Define the <guimenu>Multicast Address</guimenu>.
       </para>
      </step>
      <step>
       <para>
        Define the <guimenu>Multicast Port</guimenu>.
       </para>
       <para>
        With the values entered above, you have now defined
        <emphasis>one</emphasis> communication channel for the cluster. In
        multicast mode, the same <systemitem>bindnetaddr</systemitem>,
        <systemitem>mcastaddr</systemitem>, and
        <systemitem>mcastport</systemitem> will be used for all cluster nodes.
        All nodes in the cluster will know each other by using the same
        multicast address. For different clusters, use different multicast
        addresses.
<!--taroth 2011-10-26: for the records a statement by lmb: Setting up two or more
         clusters that use the same multicast address, but a different port, also works, 
         but is less efficient)-->
       </para>
       <figure>
        <title>&yast; Cluster&mdash;Multicast Configuration</title>
        <mediaobject>
         <imageobject role="fo">
          <imagedata fileref="yast2_cluster_comm_multicast.png" width="100%" format="PNG"/>
         </imageobject>
         <imageobject role="html">
          <imagedata fileref="yast2_cluster_comm_multicast.png" width="75%" format="PNG"/>
         </imageobject>
        </mediaobject>
       </figure>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      To use unicast:
     </para>
     <substeps performance="required">
      <step>
       <para>
        Set the <guimenu>Transport</guimenu> protocol to
        <literal>UDPU</literal>.
       </para>
      </step>
      <step>
       <para>
        Define the <guimenu>Bind Network Address</guimenu>. Set the value to
        the subnet you will use for cluster unicast.
       </para>
      </step>
      <step>
       <para>
        Define the <guimenu>Multicast Port</guimenu>.
       </para>
      </step>
      <step>
       <para>
        For unicast communication, &corosync; needs to know the IP addresses of
        all nodes in the cluster. For each node that will be part of the
        cluster, click <guimenu>Add</guimenu> and enter the following details:
       </para>
       <itemizedlist mark="bullet" spacing="normal">
        <listitem>
         <para>
          <guimenu>IP Address</guimenu>
         </para>
        </listitem>
        <listitem>
         <para>
          <guimenu>Redundant IP Address</guimenu> (only required if you use a
          second communication channel in &corosync;)
         </para>
        </listitem>
        <listitem>
         <para>
          <guimenu>Node ID</guimenu> (only required if the option <guimenu>Auto
          Generate Node ID</guimenu> is disabled)
         </para>
        </listitem>
       </itemizedlist>
       <para>
        To modify or remove any addresses of cluster members, use the
        <guimenu>Edit</guimenu> or <guimenu>Del</guimenu> buttons.
       </para>
       <figure>
        <title>&yast; Cluster&mdash;Unicast Configuration</title>
        <mediaobject>
         <imageobject role="fo">
          <imagedata fileref="yast2_cluster_comm_unicast.png" width="100%" format="PNG"/>
         </imageobject>
         <imageobject role="html">
          <imagedata fileref="yast2_cluster_comm_unicast.png" width="75%" format="PNG"/>
         </imageobject>
        </mediaobject>
       </figure>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      The option <guimenu>Auto Generate Node ID</guimenu> is enabled by
      default. If you are using IPv4 addresses, node IDs are optional but they
      are required when using IPv6 addresses. To automatically generate a
      unique ID for every cluster node (which is less error-prone than
      specifying IDs manually for each node), keep this option enabled.
     </para>
    </step>
    <step>
     <para>
      If you modified any options for an existing cluster, confirm your changes
      and close the cluster module. &yast; writes the configuration to
      <filename>/etc/corosync/corosync.conf</filename>.
     </para>
    </step>
    <step>
     <para>
      If needed, define a second communication channel as described below. Or
      click <guimenu>Next</guimenu> and proceed with
      <xref linkend="pro-ha-installation-setup-security"/>.
     </para>
    </step>
   </procedure>
<!--taroth 2010-02-05: https://fate.novell.com/307371-->
   <procedure xml:id="pro-ha-installation-setup-channel2">
    <title>Defining a Redundant Communication Channel</title>
    <para>
     If network device bonding cannot be used for any reason, the second best
     choice is to define a redundant communication channel (a second ring) in
     &corosync;. That way, two physically separate networks can be used for
     communication. In case one network fails, the cluster nodes can still
     communicate via the other network.
    </para>
    <important>
     <title>Redundant Rings and <filename>/etc/hosts</filename></title>
     <para>
      If multiple rings are configured, each node can have multiple IP
      addresses. This needs to be reflected in the
      <filename>/etc/hosts</filename> file of all nodes.
     </para>
    </important>
    <step>
     <para>
      In the &yast; cluster module, switch to the <guimenu>Communication
      Channels</guimenu> category.
     </para>
    </step>
    <step>
     <para>
      Activate <guimenu>Redundant Channel</guimenu>. The redundant channel must
      use the same protocol as the first communication channel you defined.
     </para>
    </step>
    <step>
     <para>
      If you use multicast, define the <guimenu>Bind Network Address</guimenu>,
      the <guimenu>Multicast Address</guimenu> and the <guimenu>Multicast
      Port</guimenu> for the redundant channel.
     </para>
     <para>
      If you use unicast, define the <guimenu>Bind Network Address</guimenu>,
      the <guimenu>Multicast Port</guimenu> and enter the IP addresses of all
      nodes that will be part of the cluster.
     </para>
     <para>
      Now you have defined an additional communication channel in &corosync;
      that will form a second token-passing ring. In
      <filename>/etc/corosync/corosync.conf</filename>, the primary ring (the
      first channel you have configured) gets the ringnumber
      <literal>0</literal>, the second ring (redundant channel) the ringnumber
      <literal>1</literal>.
     </para>
    </step>
    <step>
     <para>
      To tell &corosync; how and when to use the different channels, select the
      <guimenu>rrp_mode</guimenu> you want to use (<literal>active</literal> or
      <literal>passive</literal>). For more information about the modes, refer
      to <xref linkend="vle-ha-rrp"/> or click <guimenu>Help</guimenu>. As soon
      as RRP is used, the Stream Control Transmission Protocol (SCTP) is used
      for communication between the nodes (instead of TCP). The &hasi; monitors
      the status of the current rings and automatically re-enables redundant
      rings after faults. Alternatively, you can also check the ring status
      manually with <command>corosync-cfgtool</command>. View the available
      options with <option>-h</option>.
     </para>
     <para>
      If only one communication channel is defined, <guimenu>rrp_mode</guimenu>
      is automatically disabled (value <literal>none</literal>).
     </para>
    </step>
    <step>
     <para>
      If you modified any options for an existing cluster, confirm your changes
      and close the cluster module. &yast; writes the configuration to
      <filename>/etc/corosync/corosync.conf</filename>.
     </para>
    </step>
    <step>
     <para>
      For further cluster configuration, click <guimenu>Next</guimenu> and
      proceed with <xref linkend="sec-ha-installation-setup-security"/>.
     </para>
    </step>
   </procedure>
   <para>
    Find an example file for a UDP setup in
    <filename>/etc/corosync/corosync.conf.example</filename>. An example for
    UDPU setup is available in
    <filename>/etc/corosync/corosync.conf.example.udpu</filename>.
   </para>
  </section>
  <section xml:id="sec-ha-installation-setup-security">
   <title>Defining Authentication Settings</title>
   <para>
    The next step is to define the authentication settings for the cluster. You
    can use HMAC/SHA1 authentication that requires a shared secret used to
    protect and authenticate messages. The authentication key (password) you
    specify will be used on all nodes in the cluster.
   </para>
   <procedure xml:id="pro-ha-installation-setup-security">
    <title>Enabling Secure Authentication</title>
    <step>
     <para>
      In the &yast; cluster module, switch to the <guimenu>Security</guimenu>
      category.
     </para>
    </step>
    <step>
     <para>
      Activate <guimenu>Enable Security Auth</guimenu>.
     </para>
    </step>
    <step>
     <para>
      For a newly created cluster, click <guimenu>Generate Auth Key
      File</guimenu>. An authentication key is created and written to
      <filename>/etc/corosync/authkey</filename>.
     </para>
     <figure>
      <title>&yast; Cluster&mdash;Security</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="yast2_cluster_security.png" width="100%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="yast2_cluster_security.png" width="75%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
     <para>
      If you want the current machine to join an existing cluster, do not
      generate a new key file. Instead, copy the
      <filename>/etc/corosync/authkey</filename> from one of the nodes to the
      current machine (either manually or with &csync;).
     </para>
    </step>
    <step>
     <para>
      If you modified any options for an existing cluster, confirm your changes
      and close the cluster module. &yast; writes the configuration to
      <filename>/etc/corosync/corosync.conf</filename>.
     </para>
    </step>
    <step>
     <para>
      For further cluster configuration, click <guimenu>Next</guimenu> and
      proceed with <xref linkend="sec-ha-installation-setup-csync2"/>.
     </para>
    </step>
   </procedure>
  </section>
<!--taroth 2010-02-02: https://fate.novell.com/308359 (csync2)-->
  <section xml:id="sec-ha-installation-setup-csync2">
   <title>Transferring the Configuration to All Nodes</title>
   <para>
    Instead of copying the resulting configuration files to all nodes manually,
    use the <command>csync2</command> tool for replication across all nodes in
    the cluster.
   </para>
   <para>
    This requires the following basic steps:
   </para>
   <procedure>
    <step>
     <para>
      <xref linkend="pro-ha-installation-setup-csync2-yast" xrefstyle="select:title"/>.
     </para>
    </step>
    <step>
     <para>
      <xref linkend="pro-ha-installation-setup-csync2-start" xrefstyle="select:title"/>.
     </para>
    </step>
   </procedure>
   <para>
    &csync; helps you to keep track of configuration changes and to keep files
    synchronized across the cluster nodes:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      You can define a list of files that are important for operation.
     </para>
    </listitem>
    <listitem>
     <para>
      You can show changes of these files (against the other cluster nodes).
     </para>
    </listitem>
    <listitem>
     <para>
      You can synchronize the configured files with a single command.
     </para>
    </listitem>
    <listitem>
     <para>
      With a simple shell script in <filename>~/.bash_logout</filename>, you
      can be reminded about unsynchronized changes before logging out of the
      system.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    Find detailed information about &csync; at
    <link xlink:href="http://oss.linbit.com/csync2/"/> and
    <link xlink:href="http://oss.linbit.com/csync2/paper.pdf"/>.
   </para>
   <procedure xml:id="pro-ha-installation-setup-csync2-yast">
    <title>Configuring &csync; with &yast;</title>
    <step>
     <para>
      In the &yast; cluster module, switch to the <guimenu>&csync;</guimenu>
      category.
     </para>
    </step>
    <step>
     <para>
      To specify the synchronization group, click <guimenu>Add</guimenu> in the
      <guimenu>Sync Host</guimenu> group and enter the local host names of all
      nodes in your cluster. For each node, you must use exactly the strings
      that are returned by the <command>hostname</command> command.
     </para>
    </step>
    <step xml:id="step-csync2-generate-key">
     <para>
      Click <guimenu>Generate Pre-Shared-Keys</guimenu> to create a key file
      for the synchronization group. The key file is written to
      <filename>/etc/csync2/key_hagroup</filename>. After it has been created,
      it must be copied manually to all members of the cluster.
     </para>
    </step>
    <step>
     <para>
      To populate the <guimenu>Sync File</guimenu> list with the files that
      usually need to be synchronized among all nodes, click <guimenu>Add
      Suggested Files</guimenu>.
     </para>
     <figure>
      <title>&yast; Cluster&mdash;&csync;</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="yast2_cluster_sync.png" width="100%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="yast2_cluster_sync.png" width="75%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      If you want to <guimenu>Edit</guimenu>, <guimenu>Add</guimenu> or
      <guimenu>Remove</guimenu> files from the list of files to be synchronized
      use the respective buttons. You must enter the absolute path name for
      each file.
     </para>
    </step>
    <step>
     <para>
      Activate &csync; by clicking <guimenu>Turn &csync; ON</guimenu>. This
      will execute <command>chkconfig&nbsp;csync2</command> to start &csync;
      automatically at boot time.
     </para>
    </step>
    <step>
     <para>
      If you modified any options for an existing cluster, confirm your changes
      and close the cluster module. &yast; then writes the &csync;
      configuration to <filename>/etc/csync2/csync2.cfg</filename>. To start
      the synchronization process now, proceed with
      <xref linkend="pro-ha-installation-setup-csync2-start"/>.
     </para>
    </step>
    <step>
     <para>
      For further cluster configuration, click <guimenu>Next</guimenu> and
      proceed with <xref linkend="sec-ha-installation-setup-conntrackd"/>.
     </para>
    </step>
   </procedure>
   <procedure xml:id="pro-ha-installation-setup-csync2-start">
    <title>Synchronizing the Configuration Files with &csync;</title>
    <para>
     To successfully synchronize the files with &csync;, make sure that the
     following prerequisites are met:
    </para>
    <itemizedlist mark="bullet" spacing="normal">
     <listitem>
      <para>
       The same &csync; configuration is available on all nodes. Copy the file
       <filename>/etc/csync2/csync2.cfg</filename> manually to all nodes after
       you have configured it as described in
       <xref linkend="pro-ha-installation-setup-csync2-yast"/>. It is
       recommended to include this file in the list of files to be synchronized
       with &csync;.
      </para>
     </listitem>
     <listitem>
      <para>
       Copy the file <filename>/etc/csync2/key_hagroup</filename> you have
       generated on one node in <xref linkend="step-csync2-generate-key"/> to
       <emphasis>all</emphasis> nodes in the cluster. It is needed for
       authentication by &csync;. However, do <emphasis>not</emphasis>
       regenerate the file on the other nodes as it needs to be the same file
       on all nodes.
      </para>
     </listitem>
     <listitem>
      <para>
       Both &csync; and <systemitem class="daemon">xinetd</systemitem> must be
       running on <emphasis>all</emphasis> nodes.
      </para>
      <note>
       <title>Starting Services at Boot Time</title>
       <para>
        Execute the following commands on all nodes to make both services start
        automatically at boot time and to start
        <systemitem class="daemon">xinetd</systemitem> now:
       </para>
<screen>&prompt.root;chkconfig csync2 on
chkconfig xinetd on
rcxinetd start</screen>
      </note>
     </listitem>
    </itemizedlist>
    <step>
     <para>
      On the node that you want to copy the configuration
      <emphasis>from</emphasis>, execute the following command:
     </para>
<screen>&prompt.root;csync2 <option>-xv</option></screen>
     <para>
      This will synchronize all the files once by pushing them to the other
      nodes. If all files are synchronized successfully, &csync; will finish
      with no errors.
     </para>
     <para>
      If one or several files that are to be synchronized have been modified on
      other nodes (not only on the current one), &csync; will report a
      conflict. You will get an output similar to the one below:
     </para>
<screen>While syncing file /etc/corosync/corosync.conf:
ERROR from peer hex-14: File is also marked dirty here!
Finished with 1 errors.</screen>
    </step>
    <step>
     <para>
      If you are sure that the file version on the current node is the
      <quote>best</quote> one, you can resolve the conflict by forcing this
      file and resynchronizing:
     </para>
<screen>&prompt.root;csync2 -f /etc/corosync/corosync.conf
csync2 -x</screen>
    </step>
   </procedure>
   <para>
    For more information on the &csync; options, run
    <command>csync2&nbsp;</command> <option>-help</option>.
   </para>
   <note>
    <title>Pushing Synchronization After Any Changes</title>
    <para>
     &csync; only pushes changes. It does <emphasis>not</emphasis> continuously
     synchronize files between the nodes.
    </para>
    <para>
     Each time you update files that need to be synchronized, you need to push
     the changes to the other nodes: Run <command>csync2&nbsp;</command>
     <option>-xv</option> on the node where you did the changes. If you run the
     command on any of the other nodes with unchanged files, nothing will
     happen.
    </para>
   </note>
  </section>
  <section xml:id="sec-ha-installation-setup-conntrackd">
   <title>Synchronizing Connection Status Between Cluster Nodes</title>
   <para>
    To enable <emphasis>stateful</emphasis> packet inspection for iptables,
    configure and use the conntrack tools.
   </para>
   <procedure>
    <step>
     <para>
      <xref linkend="pro-ha-installation-setup-conntrackd" xrefstyle="select:title"/>.
     </para>
    </step>
    <step>
     <para>
      Configuring a resource for
      <systemitem class="daemon">conntrackd</systemitem> (class:
      <literal>ocf</literal>, provider: <literal>heartbeat</literal>). If you
      use &hawk; to add the resource, use the default values proposed by
      &hawk;.
     </para>
    </step>
   </procedure>
   <para>
    After configuring the conntrack tools, you can use them for
    <xref linkend="cha-ha-lvs" xrefstyle="select:title"/>.
   </para>
<!--from fate#311872: It supports the only FTFW syncing mode now.-->
   <procedure xml:id="pro-ha-installation-setup-conntrackd">
    <title>Configuring the <systemitem class="resource">conntrackd</systemitem> with &yast;</title>
    <para>
     Use the &yast; cluster module to configure the user-space
     <systemitem class="daemon">conntrackd</systemitem>. It needs a dedicated
     network interface that is not used for other communication channels. The
     daemon can be started via a resource agent afterward.
    </para>
    <step>
     <para>
      In the &yast; cluster module, switch to the <guimenu>Configure
      conntrackd</guimenu> category.
     </para>
    </step>
    <step>
     <para>
      Select a <guimenu>Dedicated Interface</guimenu> for synchronizing the
      connection status. The IPv4 address of the selected interface is
      automatically detected and shown in &yast;. It must already be configured
      and it must support multicast.
<!--taroth 2011-11-09: for the records, this has nothing to do with the
       corosync conf-->
     </para>
    </step>
    <step>
     <para>
      Define the <guimenu>Multicast Address</guimenu> to be used for
      synchronizing the connection status.
     </para>
    </step>
    <step>
     <para>
      In <guimenu>Group Number</guimenu>, define a numeric ID for the group to
      synchronize the connection status to.
      <remark>emap 2011-11-10: To where?
       The other nodes? - taroth: good question :), will investigate</remark>
     </para>
    </step>
    <step>
     <para>
      Click <guimenu>Generate /etc/conntrackd/conntrackd.conf</guimenu> to
      create the configuration file for
      <systemitem class="daemon">conntrackd</systemitem>.
     </para>
    </step>
    <step>
     <para>
      If you modified any options for an existing cluster, confirm your changes
      and close the cluster module.
     </para>
    </step>
    <step>
     <para>
      For further cluster configuration, click <guimenu>Next</guimenu> and
      proceed with <xref linkend="sec-ha-installation-setup-services"/>.
     </para>
    </step>
   </procedure>
   <figure>
    <title>&yast; Cluster&mdash;<systemitem class="resource">conntrackd</systemitem></title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="yast2_cluster_conntrackd.png" width="100%" format="PNG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="yast2_cluster_conntrackd.png" width="75%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
  </section>
  <section xml:id="sec-ha-installation-setup-services">
   <title>Configuring Services</title>
   <para>
    In the &yast; cluster module define whether to start certain services on a
    node at boot time. You can also use the module to start and stop the
    services manually. To bring the cluster nodes online and start the cluster
    resource manager, &ais; must be running as a service.
   </para>
   <procedure xml:id="pro-ha-installation-setup-services">
    <title>Enabling the Cluster Services</title>
    <step>
     <para>
      In the &yast; cluster module, switch to the <guimenu>Service</guimenu>
      category.
     </para>
    </step>
    <step>
     <para>
      <remark>taroth 2015-05-06: watch for answer to
      https://mailman.suse.de/mailman/private/ha-devel/2015-May/004986.html</remark>
      To start &ais; each time this cluster node is booted, select the
      respective option in the <guimenu>Booting</guimenu> group. If you select
      <guimenu>Off</guimenu> in the <guimenu>Booting</guimenu> group, you must
      start &ais; manually each time this node is booted. To start &ais;
      manually, use the <command>rcopenais&nbsp;start</command> command.
     </para>
<!-- taroth 2015-06-12: it seems there is no simple answer to the "to
       start  or not to start the cluster services" question... copied the
       following from the cloud HA implementation etherpad aspiers has mentioned
       in his reply:
       
       chkconfig openais off does not make sense when there are an odd number of nodes in the cluster, 
       because then the quorum will remain stable and the node(s) in the non-majority partition will realise 
       they do not have sufficient quorum to do anything.
      
       With 2 nodes and no SBD device, chkconfig openais off *may* make sense if the risk of a reboot loop is 
       considered high enough due to lack of redundant communication channels.  But it would incur the risk 
       of the whole system remaining down for a significant length of time until manual intervention arrives, 
       e.g. in a scenario involving total power failure followed by recovery, or a scenario where one node 
       crashes and reboots due to (say) a kernel panic, and then the other node crashes before an admin 
       has had time to restore the cluster from its degraded state (this would rely on effective 
       notifications *and* efficient admins...).  So in this configuration it would be better to only 
       disable openais startup on one of the two nodes.  
      
       If chkconfig openais off *is* done, we also need to ensure that there is a separate 'openais-shutdown' service 
       which creates /etc/init.d/rc3.d/K00openais-shutdown in order to cleanly shut down cluster resources when the 
       server is shutdown cleanly, otherwise undesired fencing will be triggered.  This also means that if 
       "chkconfig openais on" is subsequently done, "chkconfig openais-shutdown off" is required too.    
      -->
<!-- taroth 2015-05-8: tentative for fate#317778, not sure I understood
      completely what Eduardo wanted to say...  -->
     <note>
      <title>No-Start-on-Boot Parameter for &ais;</title>
      <para>
       While generally disabling the cluster service (including other
       start/stop scripts) at boot time might break the cluster configuration
       sometimes, enabling it unconditionally at boot time may also lead to
       unwanted effect with regards to fencing.
      </para>
      <para>
       To fine-tune this, insert the <literal>START_ON_BOOT</literal> parameter
       to <filename>/etc/sysconfig/openais</filename>. Setting
       <literal>START_ON_BOOT=No</literal> will prevent the &ais; service from
       starting at boot time (allowing you to start it manually whenever you
       want to start it). The default is <literal>START_ON_BOOT=Yes</literal>.
      </para>
     </note>
    </step>
    <step>
     <para>
      If you want to use the &hbgui; for configuring, managing and monitoring
      cluster resources, activate <guimenu>Enable mgmtd</guimenu>. This daemon
      is needed for the GUI.
     </para>
    </step>
    <step>
     <para>
      To start or stop &ais; immediately, click the respective button.
     </para>
    </step>
    <step>
     <para>
      To open the ports in the firewall that are needed for cluster
      communication on the current machine, activate <guimenu>Open Port in
      Firewall</guimenu>. The configuration is written to
      <filename>/etc/sysconfig/SuSEfirewall2.d/services/cluster</filename>.
     </para>
    </step>
    <step>
     <para>
      If you modified any options for an existing cluster node, confirm your
      changes and close the cluster module. Note that the configuration only
      applies to the current machine, not to all cluster nodes.
     </para>
     <para>
      If you have done the initial cluster setup exclusively with the &yast;
      cluster module, you have now completed the basic configuration steps.
      Proceed with <xref linkend="sec-ha-installation-start"/>.
     </para>
     <figure>
      <title>&yast; Cluster&mdash;Services</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="yast2_cluster_services.png" width="100%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="yast2_cluster_services.png" width="75%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
   </procedure>
  </section>
  <section xml:id="sec-ha-installation-start">
   <title>Bringing the Cluster Online</title>
   <para>
    After the initial cluster configuration is done, start the &ais;/&corosync;
    service on <emphasis>each</emphasis> cluster node to bring the stack
    online:
   </para>
   <procedure>
    <title>Starting &ais;/&corosync; and Checking the Status</title>
    <step>
     <para>
      Log in to an existing node.
     </para>
    </step>
    <step>
     <para>
      Check if the service is already running:
     </para>
<screen>&prompt.root;rcopenais status</screen>
     <para>
      If not, start &ais;/&corosync; now:
     </para>
<screen>&prompt.root;rcopenais start</screen>
    </step>
    <step>
     <para>
      Repeat the steps above for each of the cluster nodes.
     </para>
    </step>
    <step>
     <para>
      On one of the nodes, check the cluster status with the
      <command>crm&nbsp;status</command> command. If all nodes are online, the
      output should be similar to the following:
     </para>
<screen>&prompt.root;crm status
      Last updated: Thu Jul  3 11:07:10 2014
      Last change: Thu Jul  3 10:58:43 2014
      Current DC: alice (175704363) - partition with quorum
      2 Nodes configured
      0 Resources configured
      
      Online: [ alice bob ]</screen>
     <para>
      This output indicates that the cluster resource manager is started and is
      ready to manage resources.
     </para>
    </step>
   </procedure>
   <para>
    After the basic configuration is done and the nodes are online, you can
    start to configure cluster resources, using one of the cluster management
    tools like the crm shell, the &hbgui;, or the &haweb;. For more
    information, refer to the following chapters.
   </para>
  </section>
 </section>
 <section xml:id="sec-ha-installation-autoyast">
  <title>Mass Deployment with &ay;</title>
  <para>
   The following procedure is suitable for deploying cluster nodes which are
   clones of an already existing node. The cloned nodes will have the same
   packages installed and the same system configuration.
  </para>
<!--from lmb on [ha-devel] 2010-02-19: 
    That wouldn't be included - that doesn't seem to be something that
    autoyast2 can do, since it installs only a node image, replicated
    configuration files are difficult.

    However, if you clone a base install using autoyast2, I'd expect (I've
    not tested this myself!) that with SP1, it comes up to the state where
    it can receive files via csync2 from the already configured nodes, which
   should get it to the state where it can automatically join the cluster.-->
  <procedure xml:id="pro-ha-installation-clone-node">
   <title>Cloning a Cluster Node with &ay;</title>
   <important>
    <title>Identical Hardware</title>
    <para>
     This scenario assumes you are rolling out &productname; &productnumber; to
     a set of machines with identical hardware configurations.
    </para>
   </important>
   <para>
    If you need to deploy cluster nodes on non-identical hardware, refer to
    chapter <citetitle>Automated Installation</citetitle>, section
    <citetitle>Rule-Based Autoinstallation</citetitle> in the <citetitle>&sle;
    &productnumber; &deploy;</citetitle>, available at
    <link xlink:href="http://www.suse.com/doc"/>.
   </para>
   <step>
    <para>
     Make sure the node you want to clone is correctly installed and
     configured. For details, refer to
     <xref linkend="sec-ha-installation-add-on"/>, and
     <xref linkend="sec-ha-installation-setup-auto"/> or
     <xref linkend="sec-ha-installation-setup-manual"/>, respectively.
    </para>
   </step>
   <step>
    <para>
     Follow the description outlined in the <citetitle>&sle; &productnumber;
     &deploy;</citetitle> for simple mass installation. This includes the
     following basic steps:
    </para>
    <substeps performance="required">
     <step>
      <para>
       Creating an &ay; profile. Use the &ay; GUI to create and modify a
       profile based on the existing system configuration. In &ay;, choose the
       <guimenu>&ha;</guimenu> module and click the <guimenu>Clone</guimenu>
       button. If needed, adjust the configuration in the other modules and
       save the resulting control file as XML.
      </para>
     </step>
     <step>
      <para>
       Determining the source of the &ay; profile and the parameter to pass to
       the installation routines for the other nodes.
      </para>
     </step>
     <step>
      <para>
       Determining the source of the &sls; and &productname; installation data.
      </para>
     </step>
     <step>
      <para>
       Determining and setting up the boot scenario for autoinstallation.
      </para>
     </step>
     <step>
      <para>
       Passing the command line to the installation routines, either by adding
       the parameters manually or by creating an <filename>info</filename>
       file.
      </para>
     </step>
     <step>
      <para>
       Starting and monitoring the autoinstallation process.
      </para>
     </step>
    </substeps>
   </step>
  </procedure>
  <para>
   After the clone has been successfully installed, execute the following steps
   to make the cloned node join the cluster:
  </para>
  <procedure xml:id="pro-ha-installation-clone-start">
   <title>Bringing the Cloned Node Online</title>
   <step>
    <para>
     Transfer the key configuration files from the already configured nodes to
     the cloned node with &csync; as described in
     <xref linkend="sec-ha-installation-setup-csync2"/>.
    </para>
   </step>
   <step>
    <para>
     To bring the node online, start the &ais; service on the cloned node as
     described in <xref linkend="sec-ha-installation-start"/>.
    </para>
   </step>
  </procedure>
  <para>
   The cloned node will now join the cluster because the
   <filename>/etc/corosync/corosync.conf</filename> file has been applied to
   the cloned node via &csync;. The CIB is automatically synchronized among the
   cluster nodes.
  </para>
 </section>
</chapter>
