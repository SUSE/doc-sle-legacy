<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd"
[
  <!ENTITY % NOVDOC.DEACTIVATE.IDREF "INCLUDE">
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<!--
 
 Future TODOs:
 * Correct IDs
 * FATE 304867 Multilevel administration rights for CIB
   => 2010-02-23: According to Lars, not fully functionaly yet

 http://www.clusterlabs.org/wiki/Example_configurations
-->
<chapter id="cha.ha.manual_config">
 <title>Configuring and Managing Cluster Resources (Command Line)</title>
 <abstract>
  <para>
   To configure and manage cluster resources, either use the graphical user
   interface (the &hbgui;) or the <command>crm</command> command line
   utility. For the GUI approach, refer to
   <xref
    linkend="cha.ha.configuration.gui"/>.
  </para>

  <para>
   This chapter introduces <command>crm</command>, the command line tool and
   covers an overview of this tool, how to use templates, and mainly
   configuring and managing cluster resources: creating basic and advanced
   types of resources (groups and clones), configuring constraints,
   specifying failover nodes and failback nodes, configuring resource
   monitoring, starting, cleaning up or removing resources, and migrating
   resources manually.
  </para>
 </abstract>
 <note>
  <title>User Privileges</title>
  <para>
   Sufficient privileges are necessary to manage a cluster. The
   <command>crm</command> command and its subcommands need to be run either
   as &rootuser; user or as the CRM owner user (typically the user
   <systemitem
    class="username">hacluster</systemitem>).
  </para>
  <para>
   However, the <option>user</option> option allows you to run
   <command>crm</command> and its subcommands as a regular (unprivileged)
   user and to change its ID using <command>sudo</command> whenever
   necessary. For example, with the following command <command>crm</command>
   will use <systemitem class="username">hacluster</systemitem> as the
   privileged user ID:
  </para>
<screen>&prompt.root;<command>crm</command> options user hacluster</screen>
  <para>
   Note that you need to set up <filename>/etc/sudoers</filename> so that
   <command>sudo</command> does not ask for a password.
  </para>
 </note>
 <sect1 id="sec.ha.manual_config.crm">
  <title>&crmsh;&mdash;Overview</title>

  <para>
   The <command>crm</command> command has several subcommands which manage
   resources, CIBs, nodes, resource agents, and others. It offers a thorough
   help system with embedded examples. All examples follow a naming
   convention described in
   <xref linkend="app.naming" 
        xrefstyle="select:label"/>.
  </para>

  <para>
   Help can be accessed in several ways:
  </para>

  <tip>
   <title>Distinguish Between Shell Prompt and Interactive crm Prompt</title>
   <para>
    To make all the code and examples more readable, this chapter uses the
    following notations between shell prompts and the interactive crm
    prompt:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Shell prompt for user &rootuser;:
     </para>
<screen>&prompt.root;</screen>
    </listitem>
    <listitem>
     <para>
      Interactive &crmsh; prompt (displayed in green, if terminal supports
      colors):
     </para>
<screen>&prompt.crm;</screen>
    </listitem>
   </itemizedlist>
  </tip>

  <sect2 id="sec.ha.manual_config.crm.help">
   <title>Getting Help</title>
   <para>
    Help can be accessed in several ways:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      To output the usage of <command>crm</command> and its command line
      options:
     </para>
<screen>&prompt.root;<command>crm</command> --help</screen>
    </listitem>
    <listitem>
     <para>
      To give a list of all available commands:
     </para>
<screen>&prompt.root;<command>crm</command> help</screen>
    </listitem>
    <listitem>
     <para>
      To access other help sections, not just the command reference:
     </para>
<screen>&prompt.root;<command>crm</command> help topics</screen>
    </listitem>
    <listitem>
     <para>
      To view the extensive help text of the <command>configure</command>
      subcommand:
     </para>
<screen>&prompt.root;<command>crm</command> configure help</screen>
    </listitem>
    <listitem>
     <para>
      To print the syntax, its usage, and examples of a subcommand of
      <command>configure</command>:
     </para>
<screen>&prompt.root;<command>crm</command> configure help group</screen>
     <para>
      This is also possible:
     </para>
<screen>&prompt.root;<command>crm</command> help configure group</screen>
    </listitem>
   </itemizedlist>
   <para>
    Almost all output of the <command>help</command> subcommand (do not mix
    it up with the <option>--help</option> option) opens a text viewer. This
    text viewer allows you to scroll up or down and read the help text more
    comfortably. To leave the text viewer, press the <keycap>Q</keycap> key.
   </para>
   <tip id="tip.crm.tabcompletion">
    <title>Use Tab Completion in Bash and Interactive Shell</title>
    <para>
     The &crmsh; supports full tab completion in Bash directly, not only for
     the interactive shell. For example, typing <literal>crm help
     config</literal><keycap function="tab"/> will complete the word just
     like in the interactive shell.
    </para>
   </tip>
  </sect2>

  <sect2 id="sec.ha.manual_config.crm.run">
   <title>Executing &crmsh;'s Subcommands</title>
   <para>
    The <command>crm</command> command itself can be used in the following
    ways:
   </para>
   <itemizedlist>
    <listitem>
     <formalpara>
      <title>Directly</title>
      <para>
       Concatenate all subcommands to <command>crm</command>, press
       <keycap function="enter"/> and you see the output immediately. For
       example, enter <command>crm</command> <option>help ra</option> to get
       information about the <command>ra</command> subcommand (resource
       agents).
      </para>
     </formalpara>
    </listitem>
    <listitem>
     <formalpara>
      <title>As crm Shell Script</title>
      <para>
       Use <command>crm</command> and its subcommands in a script. This can
       be done in two ways:
      </para>
     </formalpara>
<screen>&prompt.root;<command>crm</command> -f script.cli
&prompt.root;<command>crm</command> &lt; script.cli</screen>
     <para>
      The script can contain any command from <command>crm</command>. For
      example:
     </para>
<screen># A small script file for crm
<command>status</command>
<command>node</command> list</screen>
     <para>
      Any line starting with the hash symbol (#) is a comment and is
      ignored. If a line is too long, insert a backslash (\) at the end and
      continue in the next line. It is recommended to indent lines that
      belong to a certain subcommand to improve readability.
     </para>
    </listitem>
    <listitem>
     <formalpara>
      <title>Interactive as Internal Shell</title>
      <para>
       Type <command>crm</command> to enter the internal shell. The prompt
       changes to <literal>&crm.live;</literal>. With
       <command>help</command> you can get an overview of the available
       subcommands. As the internal shell has different levels of
       subcommands, you can <quote>enter</quote> one by typing this
       subcommand and press <keycap function="enter"/>.
      </para>
     </formalpara>
     <para>
      For example, if you type <command>resource</command> you enter the
      resource management level. Your prompt changes to
      <literal>&crm.live;resource#</literal>. If you want to leave the
      internal shell, use the commands <command>quit</command>,
      <command>bye</command>, or <command>exit</command>. If you need to go
      one level back, use <command>back</command>, <command>up</command>,
      <command>end</command>, or <command>cd</command>.
     </para>
     <para>
      You can enter the level directly by typing <command>crm</command> and
      the respective subcommand(s) without any options and hit
      <keycap function="enter"/>.
     </para>
     <para>
      The internal shell supports also tab completion for subcommands and
      resources. Type the beginning of a command, press
      <keycap function="tab"/> and <command>crm</command> completes the
      respective object.
     </para>
    </listitem>
   </itemizedlist>
<!-- Fate#310303: -->
   <para>
    In addition to previously explained methods, &crmsh; also supports
    synchronous command execution. Use the <option>-w</option> option to
    activate it. If you have started <command>crm</command> without
    <option>-w</option>, you can enable it later with the user preference's
    <command>wait</command> set to <literal>yes</literal> (<command>options
    wait yes</command>). If this option is enabled, <command>crm</command>
    waits until the transition is finished. Whenever a transaction is
    started, dots are printed to indicate progress. Synchronous command
    execution is only applicable for commands like <command>resource
    start</command>.
   </para>
   <note>
    <title>Differentiate Between Management and Configuration Subcommands</title>
    <para>
     The <command>crm</command> tool has management capability (the
     subcommands <command>resource</command> and <command>node</command>)
     and can be used for configuration (<command>cib</command>,
     <command>configure</command>).
    </para>
   </note>
   <para>
    The following subsections give you an overview of some important aspects
    of the <command>crm</command> tool.
   </para>
  </sect2>

  <sect2 id="sec.ha.manual_config.ocf">
   <title>Displaying Information about OCF Resource Agents</title>
   <para>
    As you need to deal with resource agents in your cluster configuration
    all the time, the <command>crm</command> tool contains the
    <command>ra</command> command. Use it to show information about resource
    agents and to manage them (for additional information, see also
    <xref
     linkend="sec.ha.config.basics.raclasses"/>):
   </para>
<screen>&prompt.root;<command>crm</command> ra
&prompt.crm.ra;</screen>
   <para>
    The command <command>classes</command> lists all classes and providers:
   </para>
<screen>&prompt.crm.ra;<command>classes</command>
 lsb
 ocf / heartbeat linbit lvm2 ocfs2 pacemaker
 service
 stonith</screen>
   <para>
    To get an overview of all available resource agents for a class (and
    provider) use the <command>list</command> command:
   </para>
<screen>&prompt.crm.ra;<command>list</command> ocf
AoEtarget           AudibleAlarm        CTDB                ClusterMon
Delay               Dummy               EvmsSCC             Evmsd
Filesystem          HealthCPU           HealthSMART         ICP
IPaddr              IPaddr2             IPsrcaddr           IPv6addr
LVM                 LinuxSCSI           MailTo              ManageRAID
ManageVE            Pure-FTPd           Raid1               Route
SAPDatabase         SAPInstance         SendArp             ServeRAID
...</screen>
   <para>
    An overview of a resource agent can be viewed with
    <command>info</command>:
   </para>
<screen>&prompt.crm.ra;<command>info</command> ocf:linbit:drbd
This resource agent manages a DRBD* resource
as a master/slave resource. DRBD is a shared-nothing replicated storage
device. (ocf:linbit:drbd)

Master/Slave OCF Resource Agent for DRBD

Parameters (* denotes required, [] the default):

drbd_resource* (string): drbd resource name
    The name of the drbd resource from the drbd.conf file.

drbdconf (string, [/etc/drbd.conf]): Path to drbd.conf
    Full path to the drbd.conf file.

Operations' defaults (advisory minimum):

    start         timeout=240
    promote       timeout=90 
    demote        timeout=90 
    notify        timeout=90 
    stop          timeout=100
    monitor_Slave_0 interval=20 timeout=20 start-delay=1m
    monitor_Master_0 interval=10 timeout=20 start-delay=1m</screen>
   <para>
    Leave the viewer by pressing <keycap>Q</keycap>. Find a configuration
    example at <xref linkend="cha.ha.quickstart"/>.
   </para>
   <tip>
    <title>Use <command>crm</command> Directly</title>
    <para>
     In the former example we used the internal shell of the
     <command>crm</command> command. However, you do not necessarily need to
     use it. You get the same results if you add the respective subcommands
     to <command>crm</command>. For example, you can list all the OCF
     resource agents by entering <command>crm</command> <option>ra list
     ocf</option> in your shell.
    </para>
   </tip>
  </sect2>

  <sect2 id="sec.ha.manual_config.template">
   <title>Using Configuration Templates</title>
<!-- Info from https://bugzilla.novell.com/show_bug.cgi?id=541490 -->
   <para>
    Configuration templates are ready-made cluster configurations for the
    crm shell. Do not confuse them with the <emphasis>resource
    templates</emphasis> (as described in
    <xref 
        linkend="sec.ha.manual_config.rsc_template"/>). Those are
    templates for the <emphasis>cluster</emphasis> and not for the crm
    shell.
   </para>
   <para>
    Configuration templates require minimum effort to be tailored to the
    particular user's needs. Whenever a template creates a configuration,
    warning messages give hints which can be edited later for further
    customization.
   </para>
   <para>
    The following procedure shows how to create a simple yet functional
    Apache configuration:
   </para>
   <procedure id="pro.ha.manual_config.template">
    <step>
     <para>
      Log in as &rootuser; and start the <command>crm</command> interactive
      shell:
     </para>
<screen>&prompt.root;<command>crm</command> configure</screen>
    </step>
    <step>
     <para>
      Create a new configuration from a configuration template:
     </para>
     <substeps>
      <step>
       <para>
        Switch to the <command>template</command> subcommand:
       </para>
<screen>&prompt.crm.conf;<command>template</command></screen>
      </step>
      <step>
       <para>
        List the available configuration templates:
       </para>
<screen>&prompt.crm.conf.templ;<command>list</command> templates
gfs2-base   filesystem  virtual-ip  apache   clvm     ocfs2    gfs2</screen>
      </step>
      <step>
       <para>
        Decide which configuration template you need. As we need an Apache
        configuration, we choose the <literal>apache</literal> template and
        name it <literal>g-intranet</literal>:
       </para>
<screen>&prompt.crm.conf.templ;<command>new</command> g-intranet apache
INFO: pulling in template apache
INFO: pulling in template virtual-ip</screen>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      Define your parameters:
     </para>
     <substeps>
      <step>
       <para>
        List the configuration you have created:
       </para>
<screen>&prompt.crm.conf.templ;<command>list</command>
g-intranet</screen>
      </step>
      <step id="st.config_cli.show">
       <para>
        Display the minimum required changes that need to be filled out by
        you:
       </para>
<screen>&prompt.crm.conf.templ;<command>show</command>
ERROR: 23: required parameter ip not set
ERROR: 61: required parameter id not set
ERROR: 65: required parameter configfile not set</screen>
      </step>
      <step id="st.config_cli.edit">
       <para>
        Invoke your preferred text editor and fill out all lines that have
        been displayed as errors in
        <xref
         linkend="st.config_cli.show"/>:
       </para>
<screen>&prompt.crm.conf.templ;<command>edit</command></screen>
      </step>
<!--<step>
      <para>Edit all lines starting with <literal>%%</literal>. If you
       need an overview, use the following command in another &rootuser;
       shell:</para>
      <screen>grep -n "^%%" /root/.crmconf/intranet
23:%% ip
31:%% netmask
35:%% lvs_support
61:%% id  intranet
65:%% configfile
71:%% options
76:%% envfiles</screen>
     </step>-->
     </substeps>
    </step>
    <step>
     <para>
      Show the configuration and check whether it is valid (bold text
      depends on the configuration you have entered in
      <xref
       linkend="st.config_cli.edit" xrefstyle="select:label"/>):
     </para>
<screen>&prompt.crm.conf.templ;<command>show</command>
primitive virtual-ip ocf:heartbeat:IPaddr \
    params ip=<emphasis role="strong">"192.168.1.101"</emphasis>
primitive apache ocf:heartbeat:apache \
    params configfile=<emphasis role="strong">"/etc/apache2/httpd.conf"</emphasis>
    monitor apache 120s:60s
group <emphasis role="strong">g-intranet</emphasis> \
    apache virtual-ip</screen>
    </step>
    <step>
     <para>
      Apply the configuration:
     </para>
<screen>&prompt.crm.conf.templ;<command>apply</command>
&prompt.crm.conf;<command>cd ..</command>
&prompt.crm.conf;<command>show</command></screen>
    </step>
    <step>
     <para>
      Submit your changes to the CIB:
     </para>
<screen>&prompt.crm.conf;<command>commit</command></screen>
    </step>
   </procedure>
   <para>
    It is possible to simplify the commands even more, if you know the
    details. The above procedure can be summarized with the following
    command on the shell:
   </para>
<screen>&prompt.root;<command>crm</command> configure template \
   new g-intranet apache params \
   configfile="/etc/apache2/httpd.conf" ip="192.168.1.101"</screen>
   <para>
    If you are inside your internal <command>crm</command> shell, use the
    following command:
   </para>
<screen>&prompt.crm.conf.templ;<command
>new</command> intranet apache params \
   configfile="/etc/apache2/httpd.conf" ip="192.168.1.101"</screen>
   <para>
    However, the previous command only creates its configuration from the
    configuration template. It does not apply nor commit it to the CIB.
   </para>
  </sect2>

  <sect2 id="sec.ha.manual_config.shadowconfig">
   <title>Testing with Shadow Configuration</title>
   <para>
    A shadow configuration is used to test different configuration
    scenarios. If you have created several shadow configurations, you can
    test them one by one to see the effects of your changes.
   </para>
   <para>
    The usual process looks like this:
   </para>
   <procedure>
    <step>
     <para>
      Log in as &rootuser; and start the <command>crm</command> interactive
      shell:
     </para>
<screen>&prompt.root;<command>crm</command> configure</screen>
    </step>
    <step>
     <para>
      Create a new shadow configuration:
     </para>
<screen>&prompt.crm.conf;<command>cib</command> new myNewConfig
INFO: myNewConfig shadow CIB created</screen>
     <para>
      If you omit the name of the shadow CIB, a temporary name
      <literal>@tmp@</literal> is created.
     </para>
    </step>
    <step>
     <para>
      If you want to copy the current live configuration into your shadow
      configuration, use the following command, otherwise skip this step:
     </para>
<screen>crm(myNewConfig)# <command>cib</command> reset myNewConfig</screen>
     <para>
      The previous command makes it easier to modify any existing resources
      later.
     </para>
    </step>
    <step>
     <para>
      Make your changes as usual. After you have created the shadow
      configuration, all changes go there. To save all your changes, use the
      following command:
     </para>
<screen>crm(myNewConfig)# <command>commit</command></screen>
    </step>
    <step>
     <para>
      If you need the live cluster configuration again, switch back with the
      following command:
     </para>
<screen>crm(myNewConfig)configure# <command>cib</command> use live
&prompt.crm;</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 id="sec.ha.manual_config.debugging">
   <title>Debugging Your Configuration Changes</title>
   <para>
    Before loading your configuration changes back into the cluster, it is
    recommended to review your changes with <command>ptest</command>. The
    <command>ptest</command> command can show a diagram of actions that will
    be induced by committing the changes. You need the
    <systemitem>graphviz</systemitem> package to display the diagrams. The
    following example is a transcript, adding a monitor operation:
   </para>
<screen>&prompt.root;<command>crm</command> configure
&prompt.crm.conf;<command>show</command> fence-&node2; 
primitive fence-&node2; stonith:apcsmart \
        params hostlist="&node2;"
&prompt.crm.conf;<command>monitor</command> fence-&node2; 120m:60s
&prompt.crm.conf;<command>show</command> changed
primitive fence-&node2; stonith:apcsmart \
        params hostlist="&node2;" \
        op monitor interval="120m" timeout="60s"
&prompt.crm.conf;<emphasis role="strong">ptest</emphasis>
&prompt.crm.conf;commit</screen>
  </sect2>

  <sect2 id="sec.ha.manual_config.diagram">
   <title>Cluster Diagram</title>
   <para>
    To output a cluster diagram as shown in
    <xref 
    linkend="fig.ha.cluster.diagram"/>, use the command
    <command>crm</command> <command>configure graph</command>. It displays
    the current configuration on its current window, therefore requiring
    X11.
   </para>
   <para>
    If you prefer Scalable Vector Graphics (SVG), use the following command:
   </para>
<screen>&prompt.root;<command>crm</command> configure graph dot config.svg svg</screen>
  </sect2>
 </sect1>
<!--<sect1 id="sec.ha.manual_config.adminrights">
  <title>Setting Multilevel Administration Rights</title>
  <remark>FATE#304867 Multilevel administration rights for CIB</remark>
  <para></para>
 </sect1>-->
 <sect1 id="sec.ha.config.crm.global">
  <title>Configuring Global Cluster Options</title>

  <para>
   Global cluster options control how the cluster behaves when confronted
   with certain situations. The predefined values can usually be kept.
   However, to make key functions of your cluster work correctly, you need
   to adjust the following parameters after basic cluster setup:
  </para>

  <procedure>
   <title>Modifying Global Cluster Options With <command>crm</command></title>
   <step>
    <para>
     Log in as &rootuser; and start the <command>crm</command> tool:
    </para>
<screen>&prompt.root;<command>crm</command> configure</screen>
   </step>
   <step>
    <para>
     Use the following commands to set the options for two-node clusters
     only:
    </para>
<screen>&prompt.crm.conf;<command>property</command> no-quorum-policy=ignore
&prompt.crm.conf;<command>property</command> stonith-enabled=true</screen>
    <important>
     <title>No Support Without &stonith;</title>
     <para>
      A cluster without &stonith; is not supported.
     </para>
    </important>
   </step>
   <step>
    <para>
     Show your changes:
    </para>
<screen>&prompt.crm.conf;<command>show</command>
property $id="cib-bootstrap-options" \
   dc-version="1.1.1-530add2a3721a0ecccb24660a97dbfdaa3e68f51" \
   cluster-infrastructure="openais" \
   expected-quorum-votes="2" \
   no-quorum-policy="ignore" \
   stonith-enabled="true"</screen>
   </step>
   <step>
    <para>
     Commit your changes and exit:
    </para>
<screen>&prompt.crm.conf;<command>commit</command>
&prompt.crm.conf;<command>exit</command></screen>
   </step>
  </procedure>
 </sect1>
 <sect1 id="sec.ha.config.crm.resources">
  <title>Configuring Cluster Resources</title>

  <para>
   As a cluster administrator, you need to create cluster resources for
   every resource or application you run on servers in your cluster. Cluster
   resources can include Web sites, e-mail servers, databases, file systems,
   virtual machines, and any other server-based applications or services you
   want to make available to users at all times.
  </para>

  <para>
   For an overview of resource types you can create, refer to
   <xref linkend="sec.ha.config.basics.resources.types"/>.
  </para>

  <sect2 id="sec.ha.manual_config.create">
   <title>Creating Cluster Resources</title>
   <para>
    There are three types of RAs (Resource Agents) available with the
    cluster (for background information, see
    <xref
     linkend="sec.ha.config.basics.raclasses"/>). To add a new
    resource to the cluster, proceed as follows:
   </para>
   <procedure id="pro.ha.manual_config.create">
    <step>
     <para>
      Log in as &rootuser; and start the <command>crm</command> tool:
     </para>
<screen>&prompt.root;<command>crm</command> configure</screen>
    </step>
    <step>
     <para>
      Configure a primitive IP address:
     </para>
<screen>&prompt.crm.conf;<command>primitive</command> myIP ocf:heartbeat:IPaddr \
     params ip=127.0.0.99 op monitor interval=60s</screen>
     <para>
      The previous command configures a <quote>primitive</quote> with the
      name <literal>myIP</literal>. You need to choose a class (here
      <literal>ocf</literal>), provider (<literal>heartbeat</literal>), and
      type (<literal>IPaddr</literal>). Furthermore, this primitive expects
      other parameters like the IP address. Change the address to your
      setup.
     </para>
    </step>
    <step>
     <para>
      Display and review the changes you have made:
     </para>
<screen>&prompt.crm.conf;<command>show</command></screen>
    </step>
    <step>
     <para>
      Commit your changes to take effect:
     </para>
<screen>&prompt.crm.conf;<command>commit</command></screen>
    </step>
   </procedure>
<!--<sect2 id="sec.ha.manual_config.lsb">
    <title>LSB Initialization Scripts</title>
    <para>
    All LSB scripts are commonly found in the directory
    <filename>/etc/init.d</filename>. They must have several actions
    implemented, which are at least <literal>start</literal>,
    <literal>stop</literal>, <literal>restart</literal>,
    <literal>reload</literal>, <literal>force-reload</literal>, and
    <literal>status</literal> as explained in
    <ulink
    url="http://www.linux-foundation.org/spec/refspecs/LSB_1.3.0/gLSB/gLSB/iniscrptact.html"/>.
    </para>
    
    </sect2>-->
  </sect2>

<!-- FATE#310319 -->

  <sect2 id="sec.ha.manual_config.rsc_template">
   <title>Creating Resource Templates</title>
   <para>
    If you want to create several resources with similar configurations, a
    resource template simplifies the task. See also
    <xref linkend="sec.ha.config.basics.constraints.templates"/> for some
    basic background information. Do not confuse them with the
    <quote>normal</quote> templates from
    <xref linkend="sec.ha.manual_config.template"/>. Use the
    <command>rsc_template</command> command to get familiar with the syntax:
   </para>
<screen>&prompt.root;<command>crm</command> configure rsc_template
usage: rsc_template &lt;name> [&lt;class>:[&lt;provider>:]]&lt;type>
        [params &lt;param>=&lt;value> [&lt;param>=&lt;value>...]]
        [meta &lt;attribute>=&lt;value> [&lt;attribute>=&lt;value>...]]
        [utilization &lt;attribute>=&lt;value> [&lt;attribute>=&lt;value>...]]
        [operations id_spec
            [op op_type [&lt;attribute>=&lt;value>...] ...]]</screen>
   <para>
    For example, the following command creates a new resource template with
    the name <literal>BigVM</literal> derived from the
    <literal>ocf:heartbeat:Xen</literal> resource and some default values
    and operations:
   </para>
<screen>&prompt.crm.conf;<command>rsc_template</command> BigVM ocf:heartbeat:Xen \
   params allow_mem_management="true" \
   op monitor timeout=60s interval=15s \
   op stop timeout=10m \
   op start timeout=10m</screen>
   <para>
    Once you defined the new resource template, you can use it in primitives
    or reference it in order, colocation, or rsc_ticket constraints. To
    reference the resource template, use the <literal>@</literal> sign:
   </para>
<screen>&prompt.crm.conf;<command>primitive</command> MyVM1 @BigVM \
   params xmfile="/etc/xen/shared-vm/MyVM1" name="MyVM1"</screen>
   <para>
    The new primitive MyVM1 is going to inherit everything from the BigVM
    resource templates. For example, the equivalent of the above two would
    be:
   </para>
<screen>&prompt.crm.conf;<command>primitive</command> MyVM1 ocf:heartbeat:Xen \
   params xmfile="/etc/xen/shared-vm/MyVM1" name="MyVM1" \
   params allow_mem_management="true" \
   op monitor timeout=60s interval=15s \
   op stop timeout=10m \
   op start timeout=10m</screen>
   <para>
    If you want to overwrite some options or operations, add them to your
    (primitive) definition. For instance, the following new primitive MyVM2
    doubles the timeout for monitor operations but leaves others untouched:
   </para>
<screen>&prompt.crm.conf;<command>primitive</command> MyVM2 @BigVM \
   params xmfile="/etc/xen/shared-vm/MyVM2" name="MyVM2" \
   op monitor timeout=120s interval=30s    </screen>
   <para>
    A resource template may be referenced in constraints to stand for all
    primitives which are derived from that template. This helps to produce a
    more concise and clear cluster configuration. Resource template
    references are allowed in all constraints except location constraints.
    Colocation constraints may not contain more than one template reference.
   </para>
  </sect2>

<!--  <sect2 id="sec.ha.manual_config.example">
   <title>Example Configuration for an NFS Server</title>
   <para>
    To set up the NFS server, you need to complete the following:
   </para>
   <procedure>
    <step>
     <para>
      Configure DRBD.
     </para>
    </step>
    <step>
     <para>
      Set up a file system Resource.
     </para>
    </step>
    <step>
     <para>
      Set up the NFS server and configure the IP address.
     </para>
    </step>
   </procedure>
   <para>
    Learn how to achieve this in the following subsection.
   </para>
   <sect3 id="sec.ha.manual_config.example.drbd">
    <title>Configuring DRBD</title>
<!-\- See also DocComment#8052 -\->
    <para>
     Before starting with the DRBD &ha; configuration, set up a DRBD device
     manually. Basically this is configuring DRBD and letting it
     synchronize. The exact procedure is described in
     <xref linkend="cha.ha.drbd"/>. For now, assume that you configured a
     resource <literal>r0</literal> that may be accessed at the device
     <filename>/dev/drbd_r0</filename> on both of your cluster nodes.
    </para>
    <para>
     The DRBD resource is an OCF master/slave resource. This can be found in
     the description of the metadata of the DRBD resource agent. However, it
     is important that the actions <literal>promote</literal> and
     <literal>demote</literal> exist in the <literal>actions</literal>
     section of the metadata. These are mandatory for master/slave resources
     and commonly not available to other resources.
    </para>
    <para>
     For &ha;, master/slave resources may have multiple masters on different
     nodes. It is even possible to have a master and slave on the same node.
     Therefore, configure this resource in a way that there is exactly one
     master and one slave, each running on different nodes. Do this with the
     meta attributes of the <literal>master</literal> resource. Master/slave
     resources are special types of clone resources in &ha;. Every master
     and every slave counts as a clone.
    </para>
    <para>
     Proceed as follows to configure a DRBD resource:
    </para>
    <procedure>
     <step>
      <para>
       Open a shell and become &rootuser;.
      </para>
     </step>
     <step>
      <para>
       Enter <command>crm</command> <option>configure</option> to open the
       internal shell.
      </para>
     </step>
     <step>
      <para>
       If you have a two-node cluster, set the following properties per
       <literal>ms</literal> resource:
      </para>
<screen>&prompt.crm.conf;<command>primitive</command> my-stonith stonith:external/ipmi ...
&prompt.crm.conf;<command>ms</command> ms_drbd_r0 res_drbd_r0 meta \
   globally-unique=false ...
&prompt.crm.conf;<command>property</command> no-quorum-policy=ignore
&prompt.crm.conf;<command>property</command> stonith-enabled=true</screen>
     </step>
     <step>
      <para>
       Create a primitive DRBD resource:
      </para>
<screen>&prompt.crm.conf;<command
 >primitive</command> drbd_r0 ocf:linbit:drbd params \
 drbd drbd_resource=r0 op monitor interval="30s"</screen>
     </step>
     <step>
      <para>
       Create a master/slave resource:
      </para>
<screen>&prompt.crm.conf;<command
>ms</command> ms_drbd_r0 res_drbd_r0 meta master-max=1 \
 master-node-max=1 clone-max=2 clone-node-max=1 notify=true</screen>
     </step>
     <step>
      <para>
       Specify an colocation and order constraint:
      </para>
<screen>&prompt.crm.conf;<command
>colocation</command> fs_on_drbd_r0 inf: res_fs_r0 ms_drbd_r0:Master
&prompt.crm.conf;<command
>order</command> fs_after_drbd_r0 inf: ms_drbd_r0:promote<!-\-
      -\-> res_fs_r0:start</screen>
     </step>
     <step>
      <para>
       Display your changes with the <command>show</command> command.
      </para>
     </step>
     <step>
      <para>
       Commit your changes with the <command>commit</command> command.
      </para>
     </step>
    </procedure>
<!-\-<screen>&prompt.crm;<command>configure</command>
     &prompt.crm.conf;<command
     >primitive</command> drbd_r0 ocf:linbit:drbd params \
     drbd drbd_resource=r0 op monitor interval="30s"
     &prompt.crm.conf;<command>ms</command> drbd_resource drbd_r0 \
     meta clone_max=2 clone_node_max=1 master_max=1 master_node_max=1 notify=true
     &prompt.crm.conf;<command>commit</command></screen>-\->
   </sect3>
   <sect3 id="sec.ha.manual_config.example.filesystem">
    <title>Setting Up a File System Resource</title>
    <para>
     The <literal>filesystem</literal> resource is configured as an OCF
     primitive resource with DRBD. It has the task of mounting and
     unmounting a device to a directory on start and stop requests. In this
     case, the device is <filename>/dev/drbd_r0</filename> and the directory
     to use as mount point is <filename>/srv/failover</filename>. The file
     system used is <literal>xfs</literal>.
    </para>
    <para>
     Use the following commands in the <command>crm</command> shell to
     configure a file system resource:
    </para>
<!-\- 
     According to Philip Reiser (Linbit/DRBD inventor)
     ocf:heartbeat:Filesystem is unmaintained, therefor we need to use
     ocf:linbit:drbd
    -\->
<screen>&prompt.crm;<command>configure</command>
&prompt.crm.conf;<command>primitive</command> filesystem_resource \
    ocf:linbit:drbd \
    params device=/dev/drbd_r0 directory=/srv/failover fstype=xfs</screen>
   </sect3>
   <sect3 id="sec.ha.manual_config.example.nfs">
    <title>NFS Server and IP Address</title>
    <para>
     To make the NFS server always available at the same IP address, use an
     additional IP address as well as the ones the machines use for their
     normal operations. This IP address is then assigned to the active NFS
     server in addition to the system's IP address.
    </para>
    <para>
     The NFS server and the IP address of the NFS server should always be
     active on the same machine. In this case, the start sequence is not
     very important. They may even be started at the same time. These are
     the typical requirements for a group resource.
    </para>
    <para>
     Before starting the &ha; RA configuration, configure the NFS server
     with &yast;. Do not let the system start the NFS server. Just set up
     the configuration file. If you want to do that manually, see the manual
     page exports(5) (<command>man 5 exports</command>). The configuration
     file is <filename>/etc/exports</filename>. The NFS server is configured
     as an LSB resource.
    </para>
    <para>
     Configure the IP address completely with the &ha; RA configuration. No
     additional modification is necessary in the system. The IP address RA
     is an OCF RA.
    </para>
<screen>&prompt.crm;<command>configure</command>
&prompt.crm.conf;<command>primitive</command> nfs_resource ocf:nfsserver \
     params nfs_ip=10.10.0.1  nfs_shared_infodir=/shared
&prompt.crm.conf;<command>primitive</command> ip_resource ocf:heartbeat:IPaddr \
     params ip=10.10.0.1
&prompt.crm.conf;<command>group</command> nfs_group nfs_resource ip_resource
&prompt.crm.conf;<command>show</command>
primitive ip_res ocf:heartbeat:IPaddr \
        params ip="192.168.1.10"
primitive nfs_res ocf:heartbeat:nfsserver \
        params nfs_ip="192.168.1.10" nfs_shared_infodir="/shared"
group nfs_group nfs_res ip_res
&prompt.crm.conf;<command>commit</command>
&prompt.crm.conf;<command>end</command>
&prompt.crm;<command>quit</command></screen>
   </sect3>
  </sect2>
-->

  <sect2 id="sec.ha.manual_create.stonith">
   <title>Creating a &stonith; Resource</title>
   <para>
    From the <command>crm</command> perspective, a &stonith; device is just
    another resource. To create a &stonith; resource, proceed as follows:
   </para>
   <procedure>
    <step>
     <para>
      Log in as &rootuser; and start the <command>crm</command> interactive
      shell:
     </para>
<screen>&prompt.root;<command>crm</command> configure</screen>
    </step>
    <step>
     <para>
      Get a list of all &stonith; types with the following command:
     </para>
<screen>&prompt.crm;<command>ra</command> list stonith
apcmaster                  apcmastersnmp              apcsmart
baytech                    bladehpi                   cyclades
drac3                      external/drac5             external/dracmc-telnet
external/hetzner           external/hmchttp           external/ibmrsa
external/ibmrsa-telnet     external/ipmi              external/ippower9258
external/kdumpcheck        external/libvirt           external/nut
external/rackpdu           external/riloe             external/sbd
external/vcenter           external/vmware            external/xen0
external/xen0-ha           fence_legacy               ibmhmc
ipmilan                    meatware                   nw_rpc100s
rcd_serial                 rps10                      suicide
wti_mpc                    wti_nps</screen>
    </step>
    <step id="st.ha.manual_create.stonith.type">
     <para>
      Choose a &stonith; type from the above list and view the list of
      possible options. Use the following command:
     </para>
<screen>&prompt.crm;<command>ra</command> info stonith:external/ipmi
IPMI STONITH external device (stonith:external/ipmi)

ipmitool based power management. Apparently, the power off
method of ipmitool is intercepted by ACPI which then makes
a regular shutdown. If case of a split brain on a two-node
it may happen that no node survives. For two-node clusters
use only the reset method.

Parameters (* denotes required, [] the default):

hostname (string): Hostname
    The name of the host to be managed by this STONITH device.
...<!--
ipaddr (string): IP Address
    The IP address of the STONITH device.

userid (string): Login
    The username used for logging in to the STONITH device.

passwd (string): Password
    The password used for logging in to the STONITH device.

interface (string, [lan]): IPMI interface
    IPMI interface to use, such as "lan" or "lanplus".

stonith-timeout (time, [60s]):
    How long to wait for the STONITH action to complete. Overrides the stonith-timeout cluster property

priority (integer, [0]):
    The priority of the stonith resource. The lower the number, the higher the priority.

Operations' defaults (advisory minimum):

    start         timeout=15
    stop          timeout=15
    status        timeout=15
    monitor_0     interval=15 timeout=15 start-delay=15--></screen>
    </step>
    <step>
     <para>
      Create the &stonith; resource with the <literal>stonith</literal>
      class, the type you have chosen in
      <xref linkend="st.ha.manual_create.stonith.type" xrefstyle="select:label nopage"
      />,
      and the respective parameters if needed, for example:
     </para>
<screen>&prompt.crm;<command>configure</command>
&prompt.crm.conf;<command>primitive</command> my-stonith stonith:external/ipmi \
    params hostname="&node1;" \
    ipaddr="&subnetI;.221" \
    userid="admin" passwd="secret" \
    op monitor interval=60m timeout=120s  </screen>
    </step>
   </procedure>
  </sect2>

  <sect2 id="sec.ha.manual_config.constraints">
   <title>Configuring Resource Constraints</title>
   <para>
    Having all the resources configured is only one part of the job. Even if
    the cluster knows all needed resources, it might still not be able to
    handle them correctly. For example, try not to mount the file system on
    the slave node of DRBD (in fact, this would fail with DRBD). Define
    constraints to make these kind of information available to the cluster.
   </para>
   <para>
    For more information about constraints, see
    <xref
    linkend="sec.ha.config.basics.constraints"/>.
   </para>
   <sect3 id="sec.ha.manual_config.constraints.locational">
    <title>Locational Constraints</title>
    <para>
     The <command>location</command> command defines on which nodes a
     resource may be run, may not be run or is preferred to be run.
    </para>
    <para>
     This type of constraint may be added multiple times for each resource.
     All <literal>location</literal> constraints are evaluated for a given
     resource. A simple example that expresses a preference to run the
     resource <literal>fs1</literal> on the node with the name
     <systemitem class="server">&node1;</systemitem> to 100 would be the
     following:
    </para>
<!-- 
     location ID RSC {node_pref|rules}
     
     Grammar:
     node_pref :: <score>: <node>
     
     rules ::
     rule [id_spec] [$role=<role>] <score>: <expression>
     [rule [id_spec] [$role=<role>] <score>: <expression> ...]
     
     id_spec :: $id=<id> | $id-ref=<id>
     score :: <number> | <attribute> | [-]inf
     expression :: <single_exp> [bool_op <simple_exp> ...]
     | <date_expr>
     bool_op :: or | and
     single_exp :: <attribute> [type:]<binary_op> <value>
     | <unary_op> <attribute>
     type :: string | version | number
     binary_op :: lt | gt | lte | gte | eq | ne
     unary_op :: defined | not_defined
     
     date_expr :: date_op <start> [<end>]  (TBD) 
    -->
<screen>&prompt.crm.conf;<command>location</command> loc-fs1 fs1 100: &node1;</screen>
    <para>
     Another example is a location with pingd:
    </para>
<screen>&prompt.crm.conf;<command>primitive</command> pingd pingd \
    params name=pingd dampen=5s multiplier=100 host_list="r1 r2"
&prompt.crm.conf;<command>location</command> loc-node_pref internal_www \
    rule 50: #uname eq &node1; \
    rule pingd: defined pingd</screen>
    <para>
     Another use case for location constraints are grouping primitives as a
     <emphasis>resource set</emphasis>. This can be useful if several
     resources depend on, for example, a ping attribute for network
     connectivity. In former times, the <literal>-inf/ping</literal> rules
     needed to be duplicated several times in the configuration, making it
     unnecessarily complex.
    </para>
    <para>
     The following example creates a resource set
     <varname>loc-&node1;</varname>, referencing the virtual IP addresses
     <varname>vip1</varname> and <varname>vip2</varname>:
    </para>
<screen>&prompt.crm.conf;<command>primitive</command> vip1 ocf:heartbeat:IPaddr2 params ip=&subnetI;.5
&prompt.crm.conf;<command>primitive</command> vip1 ocf:heartbeat:IPaddr2 params ip=&subnetI;.6
&prompt.crm.conf;<command>location</command> loc-&node1; { vip1 vip2 } inf: &node1; </screen>
    <para>
     In some cases it is much more efficient and convenient to use resource
     patterns for your <command>location</command> command. A resource
     pattern is a regular expression between two slashes. For example, the
     above virtual IP addresses can be all matched with the following:
    </para>
<screen>&prompt.crm.conf;<command>location</command>  loc-&node1; /vip.*/ inf: &node1;</screen>
   </sect3>
   <sect3 id="sec.ha.manual_config.constraints.collocational">
    <title>Colocational Constraints</title>
    <para>
     The <command>colocation</command> command is used to define what
     resources should run on the same or on different hosts.
    </para>
    <para>
     It is only possible to set a score of either +inf or -inf, defining
     resources that must always or must never run on the same node. It is
     also possible to use non-infinite scores. In that case the colocation
     is called <emphasis>advisory</emphasis> and the cluster may decide not
     to follow them in favor of not stopping other resources if there is a
     conflict.
    </para>
    <para>
     For example, to run the resources with the IDs
     <literal>filesystem_resource</literal> and <literal>nfs_group</literal>
     always on the same host, use the following constraint:
    </para>
<!-- 
     colocation ID SCORE: RSC[:ROLE] RSC[:ROLE]
     
     Example: colocation dummy_and_apache -inf: apache dummy
    -->
<screen>&prompt.crm.conf;<command>colocation</command> nfs_on_filesystem inf: nfs_group filesystem_resource</screen>
    <para>
     For a master slave configuration, it is necessary to know if the
     current node is a master in addition to running the resource locally.
    </para>
   </sect3>
   <sect3 id="sec.ha.manual_config.constraints.weak-bond">
    <title>Collocating Sets for Resources Without Dependency</title>
<!-- FATE#314917 -->
    <para>
     Sometimes it is useful to be able to place a group of resources on the
     same node (defining a colocation constraint), but without having hard
     dependencies between the resources.
    </para>
    <para>
     Use the command <command>weak-bond</command> if you want to place
     resources on the same node, but without any action if one of them
     fails.
    </para>
<screen>&prompt.root;<command>crm</command> configure assist weak-bond RES1 RES2</screen>
    <para>
     The implementation of <command>weak-bond</command> creates a dummy
     resource and a colocation constraint with the given resources
     automatically.
    </para>
   </sect3>
   <sect3 id="sec.ha.manual_config.constraints.ordering">
    <title>Ordering Constraints</title>
    <para>
     The <command>order</command> command defines a sequence of action.
    </para>
    <para>
     Sometimes it is necessary to provide an order of resource actions or
     operations. For example, you cannot mount a file system before the
     device is available to a system. Ordering constraints can be used to
     start or stop a service right before or after a different resource
     meets a special condition, such as being started, stopped, or promoted
     to master.
    </para>
    <para>
     Use the following command in the <command>crm</command> shell to
     configure an ordering constraint:
    </para>
<!-- order ID score-type: FIRST-RSC[:ACTION] THEN-RSC[:ACTION] 
     
     score-type :: advisory | mandatory | <score>
     
     Example: order c_apache_1 mandatory: apache:start ip_1
    -->
<screen>&prompt.crm.conf;<command
 >order</command> nfs_after_filesystem mandatory: filesystem_resource nfs_group</screen>
   </sect3>
   <sect3 id="sec.ha.manual_config.constraints.example">
    <title>Constraints for the Example Configuration</title>
    <para>
     The example used for this section would not work without additional
     constraints. It is essential that all resources run on the same machine
     as the master of the DRBD resource. The DRBD resource must be master
     before any other resource starts. Trying to mount the DRBD device when
     it is not the master simply fails. The following constraints must be
     fulfilled:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       The file system must always be on the same node as the master of the
       DRBD resource.
      </para>
<screen>&prompt.crm.conf;<command>colocation</command> filesystem_on_master inf: \
    filesystem_resource drbd_resource:Master</screen>
     </listitem>
     <listitem>
      <para>
       The NFS server and the IP address must be on the same node as the
       file system.
      </para>
<screen>&prompt.crm.conf;<command>colocation</command> nfs_with_fs inf: \
   nfs_group filesystem_resource</screen>
     </listitem>
     <listitem>
      <para>
       The NFS server and the IP address start after the file system is
       mounted:
      </para>
<screen>&prompt.crm.conf;<command>order</command> nfs_second mandatory: \
   filesystem_resource:start nfs_group</screen>
     </listitem>
     <listitem>
      <para>
       The file system must be mounted on a node after the DRBD resource is
       promoted to master on this node.
      </para>
<screen>&prompt.crm.conf;<command>order</command> drbd_first inf: \
    drbd_resource:promote filesystem_resource:start</screen>
     </listitem>
    </itemizedlist>
   </sect3>
  </sect2>

  <sect2 id="sec.ha.manual_config.failover">
   <title>Specifying Resource Failover Nodes</title>
   <para>
    To determine a resource failover, use the meta attribute
    migration-threshold. In case failcount exceeds migration-threshold on
    all nodes, the resource will remain stopped. For example:
   </para>
<!-- Example (rsc is r1): -->
<screen>&prompt.crm.conf;<command>location</command> rsc1-&node1; rsc1 100: &node1;</screen>
   <para>
    Normally, rsc1 prefers to run on &node1;. If it fails there,
    migration-threshold is checked and compared to the failcount. If
    failcount >= migration-threshold then it is migrated to the node with
    the next best preference.
   </para>
   <para>
    Start failures set the failcount to inf depend on the
    <option>start-failure-is-fatal</option> option. Stop failures cause
    fencing. If there is no STONITH defined, the resource will not migrate.
   </para>
   <para>
    For an overview, refer to
    <xref
    linkend="sec.ha.config.basics.failover"/>.
   </para>
  </sect2>

  <sect2 id="sec.ha.manual_config.failback">
   <title>Specifying Resource Failback Nodes (Resource Stickiness)</title>
   &failback-nodes;
   <para>
    For an overview, refer to
    <xref
    linkend="sec.ha.config.basics.failback"/>.
   </para>
  </sect2>

  <sect2 id="sec.ha.manual_config.utilization">
   <title>Configuring Placement of Resources Based on Load Impact</title>
   <para>
    Some resources may have specific capacity requirements such as minimum
    amount of memory. Otherwise, they may fail to start completely or run
    with degraded performance.
   </para>
   <para>
    To take this into account, the &hasi; allows you to specify the
    following parameters:
   </para>
   <remark>dejan 2011-11-24: It is not clear whether location 
      constraints have any influence on resource placement in case 
      placement-strategy is set to something other than default. 
      Or is it explained elsewhere?</remark>
   <orderedlist>
    <listitem>
     <para>
      The capacity a certain node <emphasis>provides</emphasis>.
     </para>
    </listitem>
    <listitem>
     <para>
      The capacity a certain resource <emphasis>requires</emphasis>.
     </para>
    </listitem>
    <listitem>
     <para>
      An overall strategy for placement of resources.
     </para>
    </listitem>
   </orderedlist>
   <para>
    For detailed background information about the parameters and a
    configuration example, refer to
    <xref linkend="sec.ha.config.basics.utilization"/>.
   </para>
   <para>
    To configure the resource's requirements and the capacity a node
    provides, use utilization attributes.
<!-- as described in
    <xref linkend="pro.ha.config.gui.capacity"/>.-->
    <remark>taroth 2013-12-04:
     todo - check if this can be done with hawk and adjust link accordingly</remark>
    You can name the utilization attributes according to your preferences
    and define as many name/value pairs as your configuration needs. In
    certain cases, some agents update the utilization themselves, for
    example the <systemitem class="resource">VirtualDomain</systemitem>.
   </para>
   <para>
    In the following example, we assume that you already have a basic
    configuration of cluster nodes and resources. You now additionally want
    to configure the capacities a certain node provides and the capacity a
    certain resource requires.
<!--The procedure of adding utilization
     attributes is basically the same and only differs in
     <xref
      linkend="step.ha.config.gui.capacity.node"/> and
     <xref
      linkend="step.ha.config.gui.capacity.resource"/>.-->
   </para>
   <procedure>
    <title>Adding Or Modifying Utilization Attributes With <command>crm</command></title>
    <step>
     <para>
      Log in as &rootuser; and start the <command>crm</command> interactive
      shell:
     </para>
<screen>&prompt.root;<command>crm</command> configure</screen>
    </step>
    <step>
     <para>
      To specify the capacity a node <emphasis>provides</emphasis>, use the
      following command and replace the placeholder
      <replaceable>NODE_1</replaceable> with the name of your node:
     </para>
<screen>&prompt.crm.conf;<command>node</command>
<!--
     --><replaceable>NODE_1</replaceable> utilization memory=16384<!--
      --> cpu=8</screen>
     <para>
      With these values, <replaceable>NODE_1</replaceable> would be assumed
      to provide 16GB of memory and 8 CPU cores to resources.
     </para>
    </step>
    <step>
     <para>
      To specify the capacity a resource <emphasis>requires</emphasis>, use:
     </para>
<screen>&prompt.crm.conf;<command>primitive</command> xen1 ocf:heartbeat:Xen ... \
     utilization memory=4096 cpu=4</screen>
     <para>
      This would make the resource consume 4096 of those memory units from
      nodeA, and 4 of the CPU units.
     </para>
    </step>
    <step>
     <para>
      Configure the placement strategy with the <command>property</command>
      command:
     </para>
<screen>&prompt.crm.conf;<command>property</command> ...</screen>
     <para>
      The following values are available:
     </para>
     &placement-strategy-values;
    </step>
    <step>
     <para>
      Commit your changes before leaving &crmsh;:
     </para>
<screen>&prompt.crm.conf;<command>commit</command></screen>
    </step>
   </procedure>
   <para>
    The following example demonstrates a three node cluster of equal nodes,
    with 4 virtual machines:
   </para>
<screen>&prompt.crm.conf;<command>node</command> &node1; utilization memory="4000"
&prompt.crm.conf;<command>node</command> &node2; utilization memory="4000"
&prompt.crm.conf;<command>node</command> &node3; utilization memory="4000"
&prompt.crm.conf;<command>primitive</command> xenA ocf:heartbeat:Xen \
    utilization hv_memory="3500" meta priority="10" \
    params xmfile="/etc/xen/shared-vm/vm1"
&prompt.crm.conf;<command>primitive</command> xenB ocf:heartbeat:Xen \
    utilization hv_memory="2000" meta priority="1" \
    params xmfile="/etc/xen/shared-vm/vm2"
&prompt.crm.conf;<command>primitive</command> xenC ocf:heartbeat:Xen \
    utilization hv_memory="2000" meta priority="1" \
    params xmfile="/etc/xen/shared-vm/vm3"
&prompt.crm.conf;<command>primitive</command> xenD ocf:heartbeat:Xen \
    utilization hv_memory="1000" meta priority="5" \
    params xmfile="/etc/xen/shared-vm/vm4"
&prompt.crm.conf;<command>property</command> placement-strategy="minimal"</screen>
   <para>
    With all three nodes up, xenA will be placed onto a node first, followed
    by xenD. xenB and xenC would either be allocated together or one of them
    with xenD.
   </para>
   <para>
    If one node failed, too little total memory would be available to host
    them all. xenA would be ensured to be allocated, as would xenD. However,
    only one of xenB or xenC could still be placed, and since their priority
    is equal, the result is not defined yet. To resolve this ambiguity as
    well, you would need to set a higher priority for either one.
   </para>
  </sect2>

  <sect2 id="sec.ha.manual_config.monitor">
   <title>Configuring Resource Monitoring</title>
<!--<remark>Add the new monitor command: $ crm configure monitor usage: monitor &lt;rsc>[:&lt;role>]
    &lt;interval>[:&lt;timeout>] </remark>-->
   <para>
    To monitor a resource, there are two possibilities: either define a
    monitor operation with the <command>op</command> keyword or use the
    <command>monitor</command> command. The following example configures an
    Apache resource and monitors it every 60 seconds with the
    <literal>op</literal> keyword:
   </para>
<screen>&prompt.crm.conf;<command>primitive</command> apache apache \
  params ... \
  <emphasis>op monitor interval=60s timeout=30s</emphasis></screen>
   <para>
    The same can be done with:
   </para>
<screen>&prompt.crm.conf;<command>primitive</command> apache apache \
   params ...
&prompt.crm.conf;<command>monitor</command> apache 60s:30s</screen>
   <para>
    For an overview, refer to
    <xref
    linkend="sec.ha.config.basics.monitoring"/>.
   </para>
  </sect2>

  <sect2 id="sec.ha.manual_config.group">
   <title>Configuring a Cluster Resource Group</title>
<!--Text mostly copied from "Advanced Configuration"-->
   <para>
    One of the most common elements of a cluster is a set of resources that
    needs to be located together. Start sequentially and stop in the reverse
    order. To simplify this configuration we support the concept of groups.
    The following example creates two primitives (an IP address and an
    e-mail resource):
   </para>
   <procedure>
    <step>
     <para>
      Run the <command>crm</command> command as system administrator. The
      prompt changes to <literal>&crm.live;</literal>.
     </para>
    </step>
    <step>
     <para>
      Configure the primitives:
     </para>
<screen>&prompt.crm;<command>configure</command>
&prompt.crm.conf;<command>primitive</command> Public-IP ocf:IPaddr:heartbeat \
   params ip=1.2.3.4 id=p.public-ip
&prompt.crm.conf;<command>primitive</command> Email lsb:exim \
   params id=p.lsb-exim</screen>
    </step>
    <step>
     <para>
      Group the primitives with their relevant identifiers in the correct
      order:
     </para>
<screen>&prompt.crm.conf;<command>group</command> g-shortcut Public-IP Email</screen>
    </step>
   </procedure>
   <remark>toms 2013-03-28: FATE#313193:</remark>
   <para>
    To change the order of a group member, use the
    <command>modgroup</command> command from the
    <command>configure</command> subcommand. Use the following commands to
    move the primitive <literal>Email</literal> before
    <literal>Public-IP</literal>. (This is just to demonstrate the feature):
   </para>
<screen>&prompt.crm.conf;<command>modgroup</command
 > g-shortcut add p.lsb-exim before p.public-ip</screen>
   <para>
    In case you want to remove a resource from a group (for example,
    <literal>Email</literal>), use this command:
   </para>
<screen>&prompt.crm.conf;<command>modgroup</command
 > g-shortcut remove p.lsb-exim</screen>
   <para>
    For an overview, refer to
    <xref
    linkend="sec.ha.config.basics.resources.advanced.groups"/>.
   </para>
  </sect2>

  <sect2 id="sec.ha.manual_config.clone">
   <title>Configuring a Clone Resource</title>
<!-- Text mostly copied from "Advanced Configuration" -->
   <para>
<!-- FIXME -->
    Clones were initially conceived as a convenient way to start N instances
    of an IP resource and have them distributed throughout the cluster for
    load balancing. They have turned out to be useful for several other
    purposes, including integrating with DLM, the fencing subsystem and
    OCFS2. You can clone any resource, provided the resource agent supports
    it.
   </para>
   <para>
    Learn more about cloned resources in
    <xref 
    linkend="sec.ha.config.basics.resources.advanced.clones"/>.
   </para>
   <sect3 id="sec.ha.manual_config.clone.anonymous">
    <title>Creating Anonymous Clone Resources</title>
    <para>
     To create an anonymous clone resource, first create a primitive
     resource and then refer to it with the <command>clone</command>
     command. Do the following:
    </para>
    <procedure>
     <step>
      <para>
       Log in as &rootuser; and start the <command>crm</command> interactive
       shell:
      </para>
<screen>&prompt.root;<command>crm</command> configure</screen>
     </step>
     <step>
      <para>
       Configure the primitive, for example:
      </para>
<screen>&prompt.crm.conf;<command>primitive</command> Apache lsb:apache</screen>
     </step>
     <step>
      <para>
       Clone the primitive:
      </para>
<screen>&prompt.crm.conf;<command>clone</command> cl-apache Apache </screen>
     </step>
    </procedure>
   </sect3>
<!-- 
    <sect2 id="sec.ha.manual_config.clone.globally" os="notdefinied">
    <title>Creating Globally Unique Clone Resources</title>
    <remark>Haven't found an example. Any idea?</remark>
    <para> FIXME </para>
    </sect2>
   -->
   <sect3 id="sec.ha.manual_config.clone.stateful">
    <title>Creating Stateful/Multi-State Clone Resources</title>
    <para>
     To create a stateful clone resource, first create a primitive resource
     and then the multi-state resource. The multi-state resource must
     support at least promote and demote operations.
    </para>
    <procedure>
     <step>
      <para>
       Log in as &rootuser; and start the <command>crm</command> interactive
       shell:
      </para>
<screen>&prompt.root;<command>crm</command> configure</screen>
     </step>
     <step>
      <para>
       Configure the primitive. Change the intervals if needed:
      </para>
<screen>&prompt.crm.conf;<command>primitive</command> my-rsc ocf:myCorp:myAppl \
    op monitor interval=60 \
    op monitor interval=61 role=Master</screen>
     </step>
     <step>
      <para>
       Create the multi-state resource:
      </para>
<screen>&prompt.crm.conf;<command>ms</command> ms-rsc my-rsc</screen>
     </step>
    </procedure>
   </sect3>
  </sect2>
 </sect1>
 <sect1 id="sec.ha.config.crm">
  <title>Managing Cluster Resources</title>

  <para>
   Apart from the possibility to configure your cluster resources, the
   <command>crm</command> tool also allows you to manage existing resources.
   The following subsections gives you an overview.
  </para>

  <sect2 id="sec.ha.manual_config.start">
   <title>Starting a New Cluster Resource</title>
   <para>
    To start a new cluster resource you need the respective identifier.
    Proceed as follows:
   </para>
   <procedure>
    <step>
     <para>
      Log in as &rootuser; and start the <command>crm</command> interactive
      shell:
     </para>
<screen>&prompt.root;<command>crm</command></screen>
    </step>
    <step>
     <para>
      Switch to the resource level:
     </para>
<screen>&prompt.crm;<command>resource</command></screen>
    </step>
    <step>
     <para>
      Start the resource with <command>start</command> and press the
      <keycap function="tab"/> key to show all known resources:
     </para>
<screen>&prompt.crm.res;<command>start</command>&nbsp;<replaceable>ID</replaceable></screen>
    </step>
   </procedure>
  </sect2>

  <sect2 id="sec.ha.manual_config.cleanup">
   <title>Cleaning Up Resources</title>
   <para>
    A resource will be automatically restarted if it fails, but each failure
    raises the resource's failcount. If a
    <literal>migration-threshold</literal> has been set for that resource,
    the node will no longer be allowed to run the resource as soon as the
    number of failures has reached the migration threshold.
   </para>
   <procedure>
    <step>
     <para>
      Open a shell and log in as user &rootuser;.
     </para>
    </step>
    <step>
     <para>
      Get a list of all your resources:
     </para>
<screen>&prompt.root;<command>crm</command> resource list
  ...
Resource Group: dlm-clvm:1
         dlm:1  (ocf::pacemaker:controld) Started 
         clvm:1 (ocf::lvm2:clvmd) Started
         cmirrord:1     (ocf::lvm2:cmirrord) Started</screen>
    </step>
    <step>
     <para>
      To clean up the resource <literal>dlm</literal>, for example:
     </para>
<screen>&prompt.root;<command>crm</command> resource cleanup dlm</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 id="sec.ha.manual_config.remove">
   <title>Removing a Cluster Resource</title>
   <para>
    Proceed as follows to remove a cluster resource:
   </para>
   <procedure>
    <step>
     <para>
      Log in as &rootuser; and start the <command>crm</command> interactive
      shell:
     </para>
<screen>&prompt.root;<command>crm</command> configure</screen>
    </step>
    <step>
     <para>
      Run the following command to get a list of your resources:
     </para>
<screen>&prompt.crm;<command>resource</command> status</screen>
     <para>
      For example, the output can look like this (whereas myIP is the
      relevant identifier of your resource):
     </para>
<screen>myIP    (ocf::IPaddr:heartbeat) ...</screen>
    </step>
    <step>
     <para>
      Delete the resource with the relevant identifier (which implies a
      <command>commit</command> too):
     </para>
<screen>&prompt.crm;<command>configure</command> delete <replaceable>YOUR_ID</replaceable></screen>
    </step>
    <step>
     <para>
      Commit the changes:
     </para>
<screen>&prompt.crm;<command>configure</command> commit</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 id="sec.ha.manual_config.migrate">
   <title>Migrating a Cluster Resource</title>
   <para>
    Although resources are configured to automatically fail over (or
    migrate) to other nodes of the cluster in the event of a hardware or
    software failure, you can also manually move a resource to another node
    in the cluster using either the &hbgui; or the command line.
   </para>
   <para>
    Use the <command>migrate</command> command for this task. For example,
    to migrate the resource <literal>ipaddress1</literal> to a cluster node
    named <systemitem class="domainname">&node2;</systemitem>, use these
    commands:
   </para>
<screen>&prompt.root;<command>crm</command> resource
&prompt.crm.res;<command>migrate</command> ipaddress1 &node2;</screen>
  </sect2>

  <sect2 id="sec.ha.manual_config.tag">
   <title>Grouping/Tagging Resources</title>
<!-- https://fate.suse.com/315101 -->
   <para>
    Tags are a way to refer to multiple resources at once, without creating
    any colocation or ordering relationship between them. This can be useful
    for grouping conceptually related resources. For example, if you have
    several resources related to a database, create a tag called
    <literal>databases</literal> and add all resources related to the
    database to this tag:
   </para>
<screen>&prompt.root;<command>crm</command> configure databases: db1 db2 db3</screen>
   <para>
    This allows you to start them all with a single command:
   </para>
<screen>&prompt.root;<command>crm</command> resource start databases</screen>
   <para>
    Similarly, you can stop them all too:
   </para>
<screen>&prompt.root;<command>crm</command> resource stop databases</screen>
   
   <note>
    <title>Upgrading the CIB Syntax Version</title>
    <para>Tags (for grouping resources) and some ACL features only work with the
     CIB syntax version <literal>pacemaker-2.0</literal> or higher. For details
     on how to check this and upgrade the CIB version, see the instructions in
     the <citetitle>&haguide;</citetitle> for &productname;
     &productnumber;, section <citetitle> Upgrading from SLE HA 11 SP3 to SLE
      HA 11 SP4</citetitle>.
    </para>
   </note>
  </sect2>

  <sect2 id="sec.ha.manual_config.cli.maint.mode">
   <title>Using Maintenance Mode</title>
<!--   <remark>toms 2013-03-28: FATE#313381</remark>-->
   
   &maint-mode-basics;
   <para>
    With regard to that, &hasi; provides <literal>maintenance</literal>
    options on several levels:
   </para>
   <variablelist>
    <varlistentry>
     <term>Applying Maintenance Mode to your Cluster</term>
     <listitem>
      <para>
       In case you want to put the whole cluster in maintenance mode, use
       the following command:
      </para>
<screen>&prompt.root;<command>crm</command> configure property maintenance-mode=true</screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Applying Maintenance Mode to Nodes</term>
     <listitem>
      <para>
<!--taroth 2014-10-01: commenting because of bnc#869684: 
       If your cluster consists of more than 3 nodes, you can 
       easily set one node to maintenance mode, while the other nodes 
       continue their normal operation.-->
       For example, to put the node <literal>&node1;</literal> into
       maintenance mode:
      </para>
<screen>&prompt.root;<command>crm</command> node maintenance &node1;</screen>
      <para>
       The <command>crm status</command> command will show the maintenance
       mode for &node1; and no more resources will be allocated to that
       node. To remove the maintenance flag from the node, use:
      </para>
<screen>&prompt.root;<command>crm</command> node ready &node1;</screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Applying Maintenance Mode to Resources</term>
     <listitem>
      <para>
       If you need to set a specific resource into maintenance mode, use the
       <command>meta</command> command. For example, to put the resource
       <literal>ipaddress</literal> into maintenance mode, enter:
      </para>
<screen>&prompt.root;<command>crm</command> meta ipaddress set maintenance true</screen>
     </listitem>
    </varlistentry>
   </variablelist>
   
   &warning-maint-mode;
   
   <para>
    For more details on what happens to the resources and the cluster while
    in maintenance mode, see
    <xref linkend="sec.ha.config.basics.maint.mode"/>.
   </para>
  </sect2>
</sect1>
<!-- FATE#309125 -->
 <sect1 id="sec.ha.config.crm.setpwd">
  <title>Setting Passwords Independent of <filename>cib.xml</filename></title>

  <para>
   In case your cluster configuration contains sensitive information, such
   as passwords, it should be stored in local files. That way, these
   parameters will never be logged or leaked in support reports.
  </para>

  <para>
   Before using <command>secret</command>, better run the
   <command>show</command> command first to get an overview of all your
   resources:
  </para>

<screen>&prompt.root;<command>crm</command> configure show
primitive mydb ocf:heartbeat:mysql \
   params replication_user=admin ...</screen>

  <para>
   If you want to set a password for the above <literal>mydb</literal>
   resource, use the following commands:
  </para>

<screen>&prompt.root;<command>crm</command> resource secret mydb set passwd linux
INFO: syncing /var/lib/heartbeat/lrm/secrets/mydb/passwd to [your node list]</screen>

  <para>
   You can get the saved password back with:
  </para>

<screen>&prompt.root;<command>crm</command> resource secret mydb show passwd
linux</screen>

  <para>
   Note that the parameters need to be synchronized between nodes; the
   <command>crm resource secret</command> command will take care of that. We
   highly recommend to only use this command to manage secret parameters.
  </para>
 </sect1>
<!-- FATE#310358, #310174, #310172 -->
 <sect1 id="sec.ha.config.crm.history">
  <title>Retrieving History Information</title>

  <para>
   Investigating the cluster history is a complex task. To simplify this
   task, the crm shell contains the <command>history</command> command with
   its subcommands. It is assumed SSH is configured correctly.
  </para>

  <para>
   Each cluster moves states, migrates resources, or starts important
   processes. All these actions can be retrieved by subcommands of
   <command>history</command>. Alternatively, use &hawk; as explained in
   <xref linkend="pro.ha.config.hawk.history.explorer"/>.
  </para>

  <para>
   By default, all <command>history</command> commands look at the events of
   the last hour. To change this time frame, use the
   <command>limit</command> subcommand. The syntax is:
  </para>

<screen>&prompt.root;<command>crm</command> history
&prompt.crm.hist;<command>limit</command> <replaceable
  >FROM_TIME</replaceable> [<replaceable>TO_TIME</replaceable>]</screen>

  <para>
   Some valid examples include:
  </para>

  <variablelist>
   <varlistentry>
    <term><command>limit</command><literal>4:00pm</literal>
    </term>
    <term><command>limit</command><literal>16:00</literal>
    </term>
    <listitem>
     <para>
      Both commands mean the same, today at 4pm.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><command>limit</command><literal>2012/01/12 6pm</literal>
    </term>
    <listitem>
     <para>
      January 12th 2012 at 6pm
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><command>limit</command><literal>"Sun 5 20:46"</literal>
    </term>
    <listitem>
     <para>
      In the current year of the current month at Sunday the 5th at 8:46pm
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

<!-- On SLE 11 HA SP2: 
    # rpm -q python-dateutil
python-dateutil-1.4.1-1.20
  -->

  <para>
   Find more examples and how to create time frames at
   <ulink
    url="http://labix.org/python-dateutil"/>.
  </para>

  <para>
   The <command>info</command> subcommand shows all the parameters which are
   covered by the <command>crm_report</command>:
  </para>

<screen>&prompt.crm.hist;<command>info</command>
Source: live
Period: 2012-01-12 14:10:56 - end
Nodes: &node1;
Groups: 
Resources:</screen>

  <para>
   To limit <command>crm_report</command> to certain parameters view the
   available options with the subcommand <command>help</command>.
  </para>

  <para>
   To narrow down the level of detail, use the subcommand
   <command>detail</command> with a level:
  </para>

<screen>&prompt.crm.hist;<command>detail</command> 2</screen>

  <para>
   The higher the number, the more detailed your report will be. Default is
   <literal>0</literal> (zero).
  </para>

  <para>
   After you have set above parameters, use <command>log</command> to show
   the log messages.
  </para>

  <para>
   To display the last transition, use the following command:
  </para>

<screen>&prompt.crm.hist;<command>transition</command> -1
INFO: fetching new logs, please wait ...</screen>

  <para>
   This command fetches the logs and runs <command>dotty</command> (from the
   <systemitem class="resource">graphviz</systemitem> package) to show the
   transition graph. The shell opens the log file which you can browse with
   the <keycap
        function="down"/> and <keycap function="up"/> cursor
   keys.
  </para>

  <para>
   If you do not want to open the transition graph, use the
   <option>nograph</option> option:
  </para>

<screen>&prompt.crm.hist;<command>transition</command> -1 nograph</screen>
 </sect1>
 <sect1 id="sec.ha.config.crm.more">
  <title>For More Information</title>

  <itemizedlist>
   <listitem>
    <para>
     The crm man page.
    </para>
   </listitem>
   <listitem>
    <para>
     Visit the upstream project documentation at
     <ulink url="http://crmsh.github.io/documentation"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     See <xref linkend="art_ha_quick_nfs"/> for an exhaustive example.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
</chapter>
