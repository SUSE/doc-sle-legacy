<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
    type="text/xml"
    title="Profiling step"
?>
<!DOCTYPE chapter
[
   <!ENTITY % entities SYSTEM "entity-decl.ent">
   %entities;
]>
<!--taroth 2010-03-19: some sections have IDs that do not match our style guide
 conventions due to the last minute changes down here, no time left to fix this now-->
<!--taroth 2012-01-16: for next revision, check completely against
    http://www.linux-ha.org/wiki/SBD_Fencing-->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.1" xml:id="cha-ha-storage-protect">
 <title>Storage Protection</title>
 <info>
  <abstract>
<!--
   toms 2010-03-02:
   This abstract has to be shortend or moved to another sect, but
   for the time being, I've inserted it here:
  -->
   <para>
    The &ha; cluster stack's highest priority is protecting the integrity of
    data. This is achieved by preventing uncoordinated concurrent access to
    data storage: For example, Ext3 file systems are only mounted once in the
    cluster, OCFS2 volumes will not be mounted unless coordination with other
    cluster nodes is available. In a well-functioning cluster Pacemaker will
    detect if resources are active beyond their concurrency limits and initiate
    recovery. Furthermore, its policy engine will never exceed these
    limitations.
   </para>

   <para>
    However, network partitioning or software malfunction could potentially
    cause scenarios where several coordinators are elected. If this so-called
    split brain scenarios were allowed to unfold, data corruption might occur.
    Hence, several layers of protection have been added to the cluster stack to
    mitigate this.
   </para>

   <para>
    The primary component contributing to this goal is IO fencing/&stonith;
    since it ensures that all other access prior to storage activation is
    terminated. Other mechanisms are cLVM2 exclusive activation or OCFS2 file
    locking support to protect your system against administrative or
    application faults. Combined appropriately for your setup, these can
    reliably prevent split brain scenarios from causing harm.
   </para>

   <para>
    This chapter describes an IO fencing mechanism that leverages the storage
    itself, followed by the description of an additional layer of protection to
    ensure exclusive storage access. These two mechanisms can be combined for
    higher levels of protection.
   </para>
  </abstract>
 </info>
 <section xml:id="sec-ha-storage-protect-fencing">
  <title>Storage-based Fencing</title>
  <para>
   You can reliably avoid split brain scenarios by using one or more &stonith;
   Block Devices (SBD), <literal>watchdog</literal> support and the
   <literal>external/sbd</literal> &stonith; agent.
  </para>
  <section xml:id="sec-ha-storage-protect-fencing-oview">
   <title>Overview</title>
   <para>
    In an environment where all nodes have access to shared storage, a small
    partition of the device is formatted for use with SBD. The size of the
    partition depends on the block size of the used disk (1&nbsp;MB for
    standard SCSI disks with 512&nbsp;Byte block size; DASD disks with
    4&nbsp;kB block size need 4&nbsp;MB). After the respective daemon is
    configured, it is brought online on each node before the rest of the
    cluster stack is started. It is terminated after all other cluster
    components have been shut down, thus ensuring that cluster resources are
    never activated without SBD supervision.
   </para>
   <para>
    The daemon automatically allocates one of the message slots on the
    partition to itself, and constantly monitors it for messages addressed to
    itself. Upon receipt of a message, the daemon immediately complies with the
    request, such as initiating a power-off or reboot cycle for fencing.
   </para>
   <para>
    The daemon constantly monitors connectivity to the storage device, and
    terminates itself in case the partition becomes unreachable. This
    guarantees that it is not disconnected from fencing messages. If the
    cluster data resides on the same logical unit in a different partition,
    this is not an additional point of failure: The work-load will terminate
    anyway if the storage connectivity has been lost.
   </para>
   <para>
    Increased protection is offered through <literal>watchdog</literal>
    support. Modern systems support a <literal>hardware watchdog</literal> that
    needs to be <quote>tickled</quote> or <quote>fed</quote> by a software
    component. The software component (usually a daemon) regularly writes a
    service pulse to the watchdog&mdash;if the daemon stops feeding the
    watchdog, the hardware will enforce a system restart. This protects against
    failures of the SBD process itself, such as dying, or becoming stuck on an
    IO error.
   </para>
   <para>
    If Pacemaker integration is activated, SBD will not self-fence if device
    majority is lost. For example, your cluster contains 3 nodes: A, B, and C.
    Because of a network split, A can only see itself while B and C can still
    communicate. In this case, there are two cluster partitions, one with
    quorum because of being the majority (B, C), and one without (A). If this
    happens while the majority of fencing devices are unreachable, node A would
    immediately commit suicide, but the nodes B and C would continue to run.
   </para>
  </section>
<!--fate#309375-->
  <section xml:id="sec-ha-storage-protect-fencing-number">
   <title>Number of SBD Devices</title>
   <para>
    SBD supports the use of 1-3 devices:
   </para>
   <variablelist>
    <varlistentry>
     <term>One Device</term>
     <listitem>
      <para>
       The most simple implementation. It is appropriate for clusters where all
       of your data is on the same shared storage.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Two Devices</term>
     <listitem>
      <para>
       This configuration is primarily useful for environments that use
       host-based mirroring but where no third storage device is available. SBD
       will not terminate itself if it loses access to one mirror leg, allowing
       the cluster to continue. However, since SBD does not have enough
       knowledge to detect an asymmetric split of the storage, it will not
       fence the other side while only one mirror leg is available. Thus, it
       cannot automatically tolerate a second failure while one of the storage
       arrays is down.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Three Devices</term>
     <listitem>
      <para>
       The most reliable configuration. It is resilient against outages of one
       device&mdash;be it because of failures or maintenance. SBD will only
       terminate itself if more than one device is lost. Fencing messages can
       be successfully be transmitted if at least two devices are still
       accessible.
      </para>
      <para>
       This configuration is suitable for more complex scenarios where storage
       is not restricted to a single array. Host-based mirroring solutions can
       have one SBD per mirror leg (not mirrored itself), and an additional
       tie-breaker on iSCSI.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </section>
  <section xml:id="sec-ha-storageprotection-fencing-setup">
   <title>Setting Up Storage-based Protection</title>
   <para>
    The following steps are necessary to set up storage-based protection:
   </para>
   <procedure>
    <step>
     <para>
      <xref linkend="pro-ha-storage-protect-sbd-create" xrefstyle="select:title"/>
     </para>
    </step>
    <step>
     <para>
      <xref linkend="pro-ha-storage-protect-watchdog" xrefstyle="select:title"/>
     </para>
    </step>
    <step>
     <para>
      <xref linkend="pro-ha-storage-protect-sbd-daemon" xrefstyle="select:title"/>
     </para>
    </step>
    <step>
     <para>
      <xref linkend="pro-ha-storage-protect-sbd-test" xrefstyle="select:title"/>
     </para>
    </step>
    <step>
     <para>
      <xref linkend="pro-ha-storage-protect-fencing" xrefstyle="select:title"/>
     </para>
    </step>
   </procedure>
   <para>
    All of the following procedures must be executed as &rootuser;. Before you
    start, make sure the following requirements are met:
   </para>
   <important>
    <title>Requirements</title>
    <itemizedlist mark="bullet" spacing="normal">
     <listitem>
      <para>
       The environment must have shared storage reachable by all nodes.
      </para>
     </listitem>
     <listitem>
      <para>
       The shared storage segment must not use host-based RAID, cLVM2, nor
       DRBD*.
       <remark>taroth 2011-11-03: todo -check with DEVs: according to bg, at least
        primary-primary should work </remark>
      </para>
     </listitem>
     <listitem>
      <para>
       However, using storage-based RAID and multipathing is recommended for
       increased reliability.
      </para>
     </listitem>
    </itemizedlist>
   </important>
   <section xml:id="pro-ha-storage-protect-sbd-create">
    <title>Creating the SBD Partition</title>
    <para>
     It is recommended to create a 1MB partition at the start of the device. If
     your SBD device resides on a multipath group, you need to adjust the
     timeouts SBD uses, as MPIO's path down detection can cause some latency.
     After the <literal>msgwait</literal> timeout, the message is assumed to
     have been delivered to the node. For multipath, this should be the time
     required for MPIO to detect a path failure and switch to the next path.
     You may need to test this in your environment. The node will terminate
     itself if the SBD daemon running on it has not updated the watchdog timer
     fast enough. Test your chosen timeouts in your specific environment. In
     case you use a multipath storage with just one SBD device, pay special
     attention to the failover delays incurred.
    </para>
    <note>
     <title>Device Name for SBD Partition</title>
     <para>
      In the following, this SBD partition is referred to by
      <filename>/dev/<replaceable>SBD</replaceable> </filename>. Replace it
      with your actual path name, for example: <filename>/dev/sdc1</filename>.
     </para>
    </note>
    <important>
     <title>Overwriting Existing Data</title>
     <para>
      Make sure the device you want to use for SBD does not hold any data. The
      <command>sbd</command> command will overwrite the device without further
      requests for confirmation.
     </para>
    </important>
    <procedure>
     <step>
      <para>
       Initialize the SBD device with the following command:
      </para>
<screen>&prompt.root;<command>sbd</command> -d /dev/<replaceable>SBD</replaceable> create</screen>
      <para>
       This will write a header to the device, and create slots for up to 255
       nodes sharing this device with default timings.
      </para>
      <para>
       If you want to use more than one device for SBD, provide the devices by
       specifying the <option>-d</option> option multiple times, for example:
      </para>
<screen>&prompt.root;<command>sbd</command> -d /dev/<replaceable>SBD1</replaceable> -d /dev/<replaceable>SBD2</replaceable> -d /dev/<replaceable>SBD3</replaceable> create</screen>
     </step>
     <step>
      <para>
       If your SBD device resides on a multipath group, adjust the timeouts SBD
       uses. This can be specified when the SBD device is initialized (all
       timeouts are given in seconds):
      </para>
<!--taroth 2010-06-22: fix for http://doccomments.provo.novell.com/admin/viewcomment/14391#-->
<screen>&prompt.root;<command>/usr/sbin/sbd</command> -d /dev/<replaceable>SBD</replaceable> -4 180<co xml:id="co-msgwait"/> -1 90<co xml:id="co-watchdog"/> create</screen>
      <calloutlist>
       <callout arearefs="co-msgwait">
        <para>
         The <option>-4</option> option is used to specify the
         <literal>msgwait</literal> timeout. In the example above, it is set to
         <literal>180</literal> seconds.
        </para>
       </callout>
       <callout arearefs="co-watchdog">
        <para>
         The <option>-1</option> option is used to specify the
         <literal>watchdog</literal> timeout. In the example above, it is set
         to <literal>90</literal> seconds.
        </para>
       </callout>
      </calloutlist>
     </step>
     <step>
      <para>
       With the following command, check what has been written to the device:
      </para>
<screen>&prompt.root;<command>sbd</command> -d /dev/<replaceable>SBD</replaceable> dump 
Header version     : 2
Number of slots    : 255
Sector size        : 512
Timeout (watchdog) : 5
Timeout (allocate) : 2
Timeout (loop)     : 1
Timeout (msgwait)  : 10</screen>
     </step>
    </procedure>
    <para>
     As you can see, the timeouts are also stored in the header, to ensure that
     all participating nodes agree on them.
    </para>
   </section>
   <section xml:id="pro-ha-storage-protect-watchdog">
    <title>Setting Up the Software Watchdog</title>
    <para>
     Watchdog will protect the system against SBD failures, if no other
     software uses it.
    </para>
    <important>
     <title>Accessing the Watchdog Timer</title>
     <para>
      No other software must access the watchdog timer. Some hardware vendors
      ship systems management software that uses the watchdog for system resets
      (for example, HP ASR daemon). Disable such software, if watchdog is used
      by SBD.
     </para>
    </important>
    <para>
     In &productname;, watchdog support in the Kernel is enabled by default: It
     ships with several different Kernel modules that provide hardware-specific
     watchdog drivers. The &hasi; uses the SBD daemon as software component
     that <quote>feeds</quote> the watchdog. If configured as described in
     <xref linkend="pro-ha-storage-protect-sbd-daemon"/>, the SBD daemon will
     start automatically when the respective node is brought online with
     <command>rcopenais&nbsp;</command> <option>start</option> .
    </para>
    <para>
     Usually, the appropriate watchdog driver for your hardware is
     automatically loaded during system boot. <literal>softdog</literal> is the
     most generic driver, but it is recommended to use a driver with actual
     hardware integration. For example:
    </para>
    <itemizedlist mark="bullet" spacing="normal">
     <listitem>
      <para>
       On HP hardware, this is the <systemitem>hpwdt</systemitem> driver.
      </para>
     </listitem>
     <listitem>
      <para>
       For systems with an Intel TCO, the <literal>iTCO_wdt</literal> driver
       can be used.
      </para>
     </listitem>
    </itemizedlist>
    <para>
     For a list of choices, refer to
     <filename>/usr/src/<replaceable>KERNEL_VERSION</replaceable>/drivers/watchdog</filename>.
     Alternatively, list the drivers that have been installed with your Kernel
     version with the following command:
    </para>
<screen>&prompt.root;<command>rpm</command> -ql kernel-<replaceable>VERSION</replaceable> | <command>grep</command> watchdog</screen>
    <para>
     As most watchdog driver names contain strings like <literal>wd</literal>,
     <literal>wdt</literal>, or <literal>dog</literal>, use the following
     command to check which driver is currently loaded:
    </para>
<screen>&prompt.root;<command>lsmod</command> | <command>egrep</command> "(wd|dog)" </screen>
    <para>
     To automatically load the watchdog driver, create the file
     <filename>/etc/modules-load.d/watchdog.conf</filename> containing a line
     with the driver name. For more information refer to the man page
     <literal>modules-load.d</literal>.
    </para>
    <para>
     If you change the timeout for watchdog, the other two values
     (<literal>msgwait</literal> and <literal>stonith-timeout</literal>) must
     be changed as well. The watchdog timeout depends mostly on your storage
     latency. This value specifies that the majority of devices must
     successfully finish their read operation within this time frame. If not,
     the node will self-fence.
    </para>
    <para>
     The following <quote>formula</quote> expresses roughly this relationship
     between these three values:
    </para>
    <example xml:id="ex-ha-storage-protect-sbd-timings">
     <title>Cluster Timings with SBD as &stonith; Device</title>
<screen>Timeout (msgwait) = (Timeout (watchdog) * 2)
stonith-timeout = Timeout (msgwait) + 20%</screen>
    </example>
    <para>
     For example, if you set the timeout watchdog to 120, you need to set the
     <literal>msgwait</literal> to 240 and the
     <literal>stonith-timeout</literal> to 288. You can check the output with
     <command>sbd</command>:
    </para>
    <remark>toms 2014-08-13: Shouldn't we use "sbd -d /dev/SBD dump" here?</remark>
<screen>&prompt.root;<command>sbd</command> -d /dev/SDB dump
==Dumping header on disk /dev/sdb
Header version     : 2.1
UUID               : 619127f4-0e06-434c-84a0-ea82036e144c
Number of slots    : 255
Sector size        : 512
Timeout (watchdog) : 20
Timeout (allocate) : 2
Timeout (loop)     : 1
Timeout (msgwait)  : 40
==Header on disk /dev/sdb is dumped</screen>
    <para>
     If you set up a new cluster, the <command>sleha-init</command> command
     takes the above considerations into account.
    </para>
    <para>
     For more details about timing variables related to SBD, see the technical
     information document <citetitle>SBD Operation Guidelines for HAE
     Clusters</citetitle>, available at
     <link xlink:href="https://www.suse.com/support/kb/doc.php?id=7011346"/>.
    </para>
   </section>
   <section xml:id="pro-ha-storage-protect-sbd-daemon">
    <title>Starting the SBD Daemon</title>
    <para>
     The SBD daemon is a critical piece of the cluster stack. It needs to be
     running when the cluster stack is running, or even when part of it has
     crashed, so that the node can be fenced.
    </para>
    <procedure>
     <step>
      <para>
       Run <command>sleha-init</command>. This script ensures that SBD is
       correctly configured and the configuration file
       <filename>/etc/sysconfig/sbd</filename> is added to the list of files
       that needs to be synchronized with &csync;.
      </para>
      <para>
       If you want to configure SBD manually, perform the following step:
      </para>
      <para>
       To make the &ais; init script start and stop SBD, edit the file
       <filename>/etc/sysconfig/sbd</filename> and search for the following
       line, replacing <replaceable>SBD</replaceable> with your SBD device:
      </para>
<screen>SBD_DEVICE="/dev/<replaceable>SBD</replaceable>"</screen>
      <para>
       If you need to specify multiple devices in the first line, separate them
       by a semicolon (the order of the devices does not matter):
      </para>
<screen>SBD_DEVICE="/dev/<replaceable>SBD1</replaceable>; /dev/<replaceable>SBD2</replaceable>; /dev/<replaceable>SBD3</replaceable>"</screen>
      <para>
       If the SBD device is not accessible, the daemon will fail to start and
       inhibit &ais; startup.
      </para>
      <note>
       <title>Starting Services at Boot Time</title>
       <para>
        If the SBD device becomes inaccessible from a node, this could cause
        the node to enter an infinite reboot cycle. This is technically correct
        behavior, but depending on your administrative policies, most likely a
        nuisance. In such cases, better do not automatically start up &ais; on
        boot.
       </para>
      </note>
     </step>
     <step>
      <para>
       Before proceeding, ensure that SBD has started on all nodes by executing
       <command>rcopenais&nbsp;</command> <option>restart</option> .
      </para>
     </step>
    </procedure>
   </section>
   <section xml:id="pro-ha-storage-protect-sbd-test">
    <title>Testing SBD</title>
    <procedure>
     <step>
      <para>
       The following command will dump the node slots and their current
       messages from the SBD device:
      </para>
<screen>&prompt.root;<command>sbd</command> -d /dev/<replaceable>SBD</replaceable> list</screen>
      <para>
       Now you should see all cluster nodes that have ever been started with
       SBD listed here, the message slot should show <literal>clear</literal>.
      </para>
     </step>
     <step>
      <para>
       Try sending a test message to one of the nodes:
      </para>
      <remark>taroth 2011-04-28: comment by ulrich windl [linux-ha list]:
      the command hung after a test message was sent. I had to stop it with ^C</remark>
<screen>&prompt.root;<command>sbd</command> -d /dev/<replaceable>SBD</replaceable> message &node1; test</screen>
     </step>
     <step>
      <para>
       The node will acknowledge the receipt of the message in the system log
       files:
      </para>
<screen>Aug 29 14:10:00 &node1; sbd: [13412]: info: Received command test from &node2;</screen>
      <para>
       This confirms that SBD is indeed up and running on the node and that it
       is ready to receive messages.
      </para>
     </step>
    </procedure>
   </section>
   <section xml:id="pro-ha-storage-protect-fencing">
    <title>Configuring the Fencing Resource</title>
    <para>
     To complete the SBD setup, configure SBD as a &stonith;/fencing mechanism
     in the CIB.
    </para>
<!--taroth 2015-06-12: https://fate.suse.com/317136: asymmetric
    stonith delay to avoid double fencing-->
    <tip>
     <title>&stonith; Configuration for 2-Node Clusters</title>
     <para>
      In two-node clusters (and other clusters where
      <literal>no-quorum-policy</literal> is set to <literal>ignore</literal>),
      mistimed fencing occurs quite frequently, because both nodes will try to
      fence each other in case of a split-brain situation. To avoid this double
      fencing,
<!--make use of a ping host, and -->
      add the <literal>pcmk_delay_max</literal> parameter to the configuration
      of the &stonith; resource.
<!--If the ping host is not available, the fencing
      action will be delayed.-->
      This gives servers with a working network card a better chance to
      survive.
     </para>
    </tip>
    <procedure>
     <step>
      <para>
       Log in to one of the nodes and start the interactive &crmsh; with
       <command>crm configure</command>.
      </para>
     </step>
     <step>
      <para>
       Enter the following:
      </para>
<!--taroth 2010-06-29: fixed bnc#617920-->
<screen>&prompt.crm.conf;<command>property</command> stonith-enabled="true"
&prompt.crm.conf;<command>property</command> stonith-timeout="40s" <co xml:id="co-ha-sbd-stonith-timeout"/>
&prompt.crm.conf;<command>primitive</command> stonith_sbd stonith:external/sbd \
   pcmk_delay_max="30" <co xml:id="co-ha-sbd-pcmk-delay-max"/></screen>
      <para>
       The resource does not need to be cloned. As node slots are allocated
       automatically, no manual host list needs to be defined.
      </para>
      <calloutlist>
       <callout arearefs="co-ha-sbd-stonith-timeout">
<!-- bnc#891346 -->
<!--Use the following formula to set the minimum value of
         <literal>stonith-timeout</literal>:</para>
         <screen>stonith-timeout >= 1.2 * msgwait</screen>-->
        <para>
         Which value to set for <literal>stonith-timeout</literal> depends on
         the <literal>msgwait</literal> timeout. The <literal>msgwait</literal>
         timeout should be longer than the maximum allowed timeout for the
         underlying IO system. For example, this is 30 seconds for plain SCSI
         disks. Provided you set the <literal>msgwait</literal> timeout value
         to 30 seconds, setting <literal>stonith-timeout</literal> to 40
         seconds is appropriate.
        </para>
       </callout>
       <callout arearefs="co-ha-sbd-pcmk-delay-max">
        <para>
         The <literal>pcmk_delay_max</literal> parameter enables a random delay
         for &stonith; actions on the fencing device. Its value specifies the
         maximum amount of time to wait before the start operation of the
         &stonith; device is executed. As it takes time to detect the ring
         failure, become the DC and to start the &stonith; resource, do not set
         this value too low (otherwise the prior DC will always start the
         fencing action first).
        </para>
       </callout>
      </calloutlist>
     </step>
     <step>
      <para>
       Commit the changes:
      </para>
<screen>&prompt.crm;<command>configure</command> commit</screen>
     </step>
     <step>
      <para>
       Disable any other fencing devices you might have configured before,
       since the SBD mechanism is used for this function now.
      </para>
     </step>
    </procedure>
    <para>
     After the resource has started, your cluster is successfully configured
     for shared-storage fencing and will use this method in case a node needs
     to be fenced.
    </para>
   </section>
   <section>
    <title>For More Information</title>
    <para>
     <link xlink:href="http://www.linux-ha.org/wiki/SBD_Fencing"/>
    </para>
   </section>
  </section>
 </section>
 <section xml:id="sec-ha-storageprotection-exstoract">
  <title>Ensuring Exclusive Storage Activation</title>
  <para>
   This section introduces <literal>sfex</literal>, an additional low-level
   mechanism to lock access to shared storage exclusively to one node. Note
   that sfex does not replace &stonith;. Since sfex requires shared storage, it
   is recommended that the <literal>external/sbd</literal> fencing mechanism
   described above is used on another partition of the storage.
  </para>
  <para>
   By design, sfex cannot be used with workloads that require concurrency (such
   as OCFS2), but serves as a layer of protection for classic failover style
   workloads. This is similar to a SCSI-2 reservation in effect, but more
   general.
  </para>
  <section xml:id="sec-ha-storageprotection-exstoract-description">
   <title>Overview</title>
   <para>
    In a shared storage environment, a small partition of the storage is set
    aside for storing one or more locks.
   </para>
   <para>
    Before acquiring protected resources, the node must first acquire the
    protecting lock. The ordering is enforced by Pacemaker, and the sfex
    component ensures that even if Pacemaker were subject to a split brain
    situation, the lock will never be granted more than once.
   </para>
   <para>
    These locks must also be refreshed periodically, so that a node's death
    does not permanently block the lock and other nodes can proceed.
   </para>
  </section>
  <section xml:id="sec-ha-storageprotection-exstoract-requirements">
   <title>Setup</title>
   <para>
    In the following, learn how to create a shared partition for use with sfex
    and how to configure a resource for the sfex lock in the CIB. A single sfex
    partition can hold any number of locks, it
    <remark>taroth 2010-03-19: again, unclear reference - what is "it"referring
     to here? clarify for next revision!</remark>
    defaults to one, and needs 1 KB of storage space allocated per lock.
   </para>
   <important>
    <title>Requirements</title>
    <itemizedlist mark="bullet" spacing="normal">
     <listitem>
      <para>
       The shared partition for sfex should be on the same logical unit as the
       data you want to protect.
      </para>
     </listitem>
     <listitem>
      <para>
       The shared sfex partition must not use host-based RAID, nor DRBD.
      </para>
     </listitem>
     <listitem>
      <para>
       Using a cLVM2 logical volume is possible.
      </para>
     </listitem>
    </itemizedlist>
   </important>
   <procedure>
    <title>Creating an sfex Partition</title>
    <step>
     <para>
      Create a shared partition for use with sfex. Note the name of this
      partition and use it as a substitute for <filename>/dev/sfex</filename>
      below.
     </para>
    </step>
    <step>
     <para>
      Create the sfex meta data with the following command:
     </para>
<screen>&prompt.root;<command>sfex_init</command> -n 1 /dev/sfex</screen>
    </step>
    <step>
     <para>
      Verify that the meta data has been created correctly:
     </para>
<screen>&prompt.root;<command>sfex_stat</command> -i 1 /dev/sfex ; echo $?</screen>
     <para>
      This should return <literal>2</literal>, since the lock is not currently
      held.
     </para>
    </step>
   </procedure>
   <procedure>
    <title>Configuring a Resource for the sfex Lock</title>
    <step>
     <para>
      The sfex lock is represented via a resource in the CIB, configured as
      follows:
     </para>
<screen>&prompt.crm.conf;<command>primitive</command> sfex_1 ocf:heartbeat:sfex \
#	params device="/dev/sfex" index="1" collision_timeout="1" \
      lock_timeout="70" monitor_interval="10" \
#	op monitor interval="10s" timeout="30s" on_fail="fence"</screen>
    </step>
    <step>
     <para>
      To protect resources via an sfex lock, create mandatory ordering and
      placement constraints between the protectees and the sfex resource. If
      the resource to be protected has the id <literal>filesystem1</literal>:
     </para>
<screen>&prompt.crm.conf;<command>order</command> order-sfex-1 inf: sfex_1 filesystem1
&prompt.crm.conf;<command>colocation</command> colo-sfex-1 inf: filesystem1 sfex_1</screen>
    </step>
    <step>
     <para>
      If using group syntax, add the sfex resource as the first resource to the
      group:
     </para>
<screen>&prompt.crm.conf;<command>group</command> LAMP sfex_1 filesystem1 apache ipaddr</screen>
    </step>
   </procedure>
  </section>
 </section>
 <section xml:id="sec-ha-storage-moreinfo">
  <title>For More Information</title>
  <para>
   See <link xlink:href="http://www.linux-ha.org/wiki/SBD_Fencing"/> and
   <command>man sbd</command>.
  </para>
 </section>
</chapter>
