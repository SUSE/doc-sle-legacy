<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
    type="text/xml"
    title="Profiling step"
?>
<!DOCTYPE chapter
[
   <!ENTITY % entities SYSTEM "entity-decl.ent">
   %entities;
]>
<!--taroth 2010-08-19: for next revision, see also Sander's book
(chapter about cLVM) for more information and details to integrate here-->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.1" xml:id="cha-ha-ocfs2">
 <title>OCFS2</title>
 <info>
  <abstract>
   <para>
    Oracle Cluster File System 2 (OCFS2) is a general-purpose journaling file
    system that has been fully integrated since the Linux 2.6 Kernel. OCFS2
    allows you to store application binary files, data files, and databases on
    devices on shared storage. All nodes in a cluster have concurrent read and
    write access to the file system. A user-space control daemon, managed via a
    clone resource, provides the integration with the HA stack, in particular
    with &ais;/&corosync; and the Distributed Lock Manager (DLM).
   </para>
  </abstract>
 </info><indexterm class="startofrange" xml:id="idx-filesystems-ocfs2">
 <primary>file systems</primary>
 <secondary>OCFS2</secondary></indexterm>
 <section xml:id="sec-ha-ocfs2-features">
  <title>Features and Benefits</title>
  <para>
   OCFS2 can be used for the following storage solutions for example:
  </para>
  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     General applications and workloads.
    </para>
   </listitem>
   <listitem>
    <para>
     &xen; image store in a cluster. &xen; virtual machines and virtual servers
     can be stored on OCFS2 volumes that are mounted by cluster servers. This
     provides quick and easy portability of &xen; virtual machines between
     servers.
    </para>
   </listitem>
   <listitem>
    <para>
     LAMP (Linux, Apache, MySQL, and PHP &verbar; Perl &verbar; Python) stacks.
    </para>
   </listitem>
  </itemizedlist>
  <para>
   As a high-performance, symmetric and parallel cluster file system, OCFS2
   supports the following functions:
  </para>
  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     An application's files are available to all nodes in the cluster. Users
     simply install it once on an OCFS2 volume in the cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     All nodes can concurrently read and write directly to storage via the
     standard file system interface, enabling easy management of applications
     that run across the cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     File access is coordinated through DLM. DLM control is good for most
     cases, but an application's design might limit scalability if it contends
     with the DLM to coordinate file access.
    </para>
   </listitem>
   <listitem>
    <para>
     Storage backup functionality is available on all back-end storage. An
     image of the shared application files can be easily created, which can
     help provide effective disaster recovery.
    </para>
   </listitem>
  </itemizedlist>
  <para>
   OCFS2 also provides the following capabilities:
  </para>
  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     Metadata caching.
    </para>
   </listitem>
   <listitem>
    <para>
     Metadata journaling.
    </para>
   </listitem>
   <listitem>
    <para>
     Cross-node file data consistency.
    </para>
   </listitem>
<!--
    <listitem>
     <para>
      A GTK GUI-based administration via the
      <command>ocfs2console</command> utility
     </para>
    </listitem> -->
<!--
    <listitem>
     <para>
      Operation as a shared-root file system
     </para>
    </listitem> -->
   <listitem>
    <para>
     Support for multiple-block sizes up to 4 KB, cluster sizes up to 1 MB, for
     a maximum volume size of 4 PB (Petabyte).
    </para>
   </listitem>
   <listitem>
    <para>
     Support for up to 32 cluster nodes.
    </para>
   </listitem>
<!-- taroth 2010-03-12: ocfs2cdsl not on our media - 
         not supported according to coly-->
<!--      
    <listitem>
     <para>
      Context-dependent symbolic link (CDSL) support for node-specific
      local files
     </para>
    </listitem> -->
   <listitem>
    <para>
     Asynchronous and direct I/O support for database files for improved
     database performance.
    </para>
   </listitem>
  </itemizedlist>
 </section>
 <section xml:id="sec-ha-ocfs2-utils">
  <title>OCFS2 Packages and Management Utilities</title>
  <para>
   The OCFS2 Kernel module (<literal>ocfs2</literal>) is installed
   automatically in the &hasi; on &slsreg; &productnumber;. To use OCFS2, make
   sure the following packages are installed on each node in the cluster:
   <systemitem class="resource">ocfs2-tools</systemitem> and the matching
   <systemitem class="resource">ocfs2-kmp-*</systemitem> packages for your
   Kernel.
  </para>
  <para>
   The <systemitem class="resource">ocfs2-tools</systemitem> package provides
   the following utilities for management of OFS2 volumes. For syntax
   information, see their man pages.
  </para>
  <table>
   <title>OCFS2 Utilities</title>
   <tgroup cols="2">
    <thead>
     <row>
      <entry>
       <para>
        OCFS2 Utility
       </para>
      </entry>
      <entry>
       <para>
        Description
       </para>
      </entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry>
       <para>
        debugfs.ocfs2
       </para>
      </entry>
      <entry>
       <para>
        Examines the state of the OCFS file system for the purpose of
        debugging.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        fsck.ocfs2
       </para>
      </entry>
      <entry>
       <para>
        Checks the file system for errors and optionally repairs errors.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        mkfs.ocfs2
       </para>
      </entry>
      <entry>
       <para>
        Creates an OCFS2 file system on a device, usually a partition on a
        shared physical or logical disk.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        mounted.ocfs2
       </para>
      </entry>
      <entry>
       <para>
        Detects and lists all OCFS2 volumes on a clustered system. Detects and
        lists all nodes on the system that have mounted an OCFS2 device or
        lists all OCFS2 devices.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        tunefs.ocfs2
       </para>
      </entry>
      <entry>
       <para>
        Changes OCFS2 file system parameters, including the volume label,
        number of node slots, journal size for all node slots, and volume size.
       </para>
      </entry>
     </row>
    </tbody>
   </tgroup>
  </table>
 </section>
 <section xml:id="sec-ha-ocfs2-create-service">
  <title>Configuring OCFS2 Services and a &stonith; Resource</title>
<!--https://bugzilla.novell.com/show_bug.cgi?id=520714-->
  <para>
   Before you can create OCFS2 volumes, you must configure the following
   resources as services in the cluster: DLM, O2CB and a &stonith; resource.
   OCFS2 uses the cluster membership services from Pacemaker which run in user
   space. Therefore, DLM and O2CB need to be configured as clone resources that
   are present on each node in the cluster.
  </para>
  <para>
   The following procedure uses the <command>crm</command> shell to configure
   the cluster resources. Alternatively, you can also use the &hbgui; to
   configure the resources.
  </para>
  <note>
   <title>DLM Resource for Both cLVM and OCFS2</title>
   <para>
    Both cLVM and OCFS2 need a DLM resource that runs on all nodes in the
    cluster and therefore usually is configured as a clone. If you have a setup
    that includes both OCFS2 and cLVM, configuring <emphasis>one</emphasis> DLM
    resource for both OCFS2 and cLVM is enough.
   </para>
  </note>
  <procedure xml:id="pro-ocfs2-resources">
   <title>Configuring DLM and O2CB Resources</title>
   <para>
    The configuration consists of a base group that includes several primitives
    and a base clone. Both base group and base clone can be used in various
    scenarios afterwards (for both OCFS2 and cLVM, for example). You only need
    to extended the base group with the respective primitives as needed. As the
    base group has internal colocation and ordering, this facilitates the
    overall setup as you do not have to specify several individual groups,
    clones and their dependencies.
   </para>
   <para>
    Follow the steps below for one node in the cluster:
   </para>
   <step>
    <para>
     Start a shell and log in as &rootuser; or equivalent.
    </para>
   </step>
   <step>
    <para>
     Run <command>crm</command> <option>configure</option>.
    </para>
   </step>
   <step>
    <para>
     Enter the following to create the primitive resources for DLM and O2CB:
    </para>
<!--Brendo, 17.11.2010: for dlm use the following params:
      params args="-q 0 -f 0 -t 1000 -a 1000"
      (see man dlm_controld) -> this keeps ocfs2 from blocking if a node is
      missing-->
<screen>&prompt.crm.conf;<command>primitive</command> dlm ocf:pacemaker:controld \
      op monitor interval="60" timeout="60"
<command>primitive</command> o2cb ocf:ocfs2:o2cb \
      op monitor interval="60" timeout="60"
</screen>
    <para>
     The <literal>dlm</literal> clone resource controls the distributed lock
     manager service and makes sure this service is started on all nodes in the
     cluster. Due to the base group's internal colocation and ordering, the
     <literal>o2cb</literal> service is only started on nodes where a copy of
     the <literal>dlm</literal> service is already running.
    </para>
   </step>
   <step>
    <para>
     Enter the following to create a base group and a base clone:
<!--Brendo, 17.11.2010: maybe only add two clones and omit the group
         clone dlm-clone dlm meta interleave="true"
         clone o2cb-clone o2cb meta interleave="true"
      -->
    </para>
<screen>&prompt.crm.conf;<command>group</command> base-group dlm o2cb 
<command>clone</command> base-clone base-group \
      meta interleave="true"</screen>
   </step>
   <step>
    <para>
     Review your changes with <command>show</command>.
    </para>
   </step>
   <step>
    <para>
     If everything is correct, submit your changes with
     <command>commit</command> and leave the crm live configuration with
     <command>exit</command>.
    </para>
   </step>
  </procedure>
  <procedure xml:id="pro-ocfs2-stonith">
   <title>Configuring a &stonith; Resource</title>
   <note>
    <title>&stonith; Device Needed</title>
    <para>
     You need to configure a fencing device. Without a &stonith; mechanism
     (like <literal>external/sbd</literal>) in place the configuration will
     fail.
    </para>
   </note>
   <step>
    <para>
     Start a shell and log in as &rootuser; or equivalent.
    </para>
   </step>
   <step>
    <para>
     Create an SBD partition as described in
     <xref linkend="pro-ha-storage-protect-sbd-create"/>.
    </para>
   </step>
   <step>
    <para>
     Run <command>crm</command> <option>configure</option>.
    </para>
   </step>
   <step>
    <para>
     Configure <literal>external/sdb</literal> as fencing device with
     <literal>/dev/sdb2</literal> being a dedicated partition on the shared
     storage for heartbeating and fencing:
    </para>
<screen>&prompt.crm.conf;<command>primitive</command> sbd_stonith stonith:external/sbd \
      params pcmk_delay_max="30" meta target-role="Started"</screen>
   </step>
   <step>
    <para>
     Review your changes with <command>show</command>.
    </para>
   </step>
   <step>
    <para>
     If everything is correct, submit your changes with
     <command>commit</command> and leave the crm live configuration with
     <command>exit</command>.
    </para>
   </step>
  </procedure>
 </section>
 <section xml:id="sec-ha-ocfs2-create">
  <title>Creating OCFS2 Volumes</title>
  <para>
   After you have configured DLM and O2CB as cluster resources as described in
   <xref linkend="sec-ha-ocfs2-create-service"/>, configure your system to use
   OCFS2 and create OCFs2 volumes.
  </para>
  <note>
   <title>OCFS2 Volumes for Application and Data Files</title>
   <para>
    We recommend that you generally store application files and data files on
    different OCFS2 volumes. If your application volumes and data volumes have
    different requirements for mounting, it is mandatory to store them on
    different volumes.
   </para>
  </note>
  <para>
   Before you begin, prepare the block devices you plan to use for your OCFS2
   volumes. Leave the devices as free space.
  </para>
  <para>
   Then create and format the OCFS2 volume with the
   <command>mkfs.ocfs2</command> as described in
   <xref linkend="pro-ocfs2-volume"/>. The most important parameters for the
   command are listed in <xref linkend="tab-ha-ofcs2-mkfs-ocfs2-params"/>. For
   more information and the command syntax, refer to the
   <command>mkfs.ocfs2</command> man page.
  </para>
  <table xml:id="tab-ha-ofcs2-mkfs-ocfs2-params">
   <title>Important OCFS2 Parameters</title>
   <tgroup cols="2">
    <thead>
     <row>
      <entry>
       <para>
        OCFS2 Parameter
       </para>
      </entry>
      <entry>
       <para>
        Description and Recommendation
       </para>
      </entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry>
       <para>
        Volume Label (<option>-L</option>)
       </para>
      </entry>
      <entry>
       <para>
        A descriptive name for the volume to make it uniquely identifiable when
        it is mounted on different nodes. Use the
        <command>tunefs.ocfs2</command> utility to modify the label as needed.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        Cluster Size (<option>-C</option>)
       </para>
      </entry>
      <entry>
       <para>
        Cluster size is the smallest unit of space allocated to a file to hold
        the data. For the available options and recommendations, refer to the
        <command>mkfs.ocfs2</command> man page.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        Number of Node Slots (<option>-N</option>)
       </para>
      </entry>
      <entry>
       <para>
        The maximum number of nodes that can concurrently mount a volume. For
        each of the nodes, OCFS2 creates separate system files, such as the
        journals, for each of the nodes. Nodes that access the volume can be a
        combination of little-endian architectures (such as x86, x86-64, and
        ia64) and big-endian architectures (such as ppc64 and s390x).
       </para>
       <para>
        Node-specific files are referred to as local files. A node slot number
        is appended to the local file. For example:
        <literal>journal:0000</literal> belongs to whatever node is assigned to
        slot number <literal>0</literal>.
       </para>
       <para>
        Set each volume's maximum number of node slots when you create it,
        according to how many nodes that you expect to concurrently mount the
        volume. Use the <command>tunefs.ocfs2</command> utility to increase the
        number of node slots as needed. Note that the value cannot be
        decreased.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        Block Size (<option>-b</option>)
       </para>
      </entry>
      <entry>
       <para>
        The smallest unit of space addressable by the file system. Specify the
        block size when you create the volume. For the available options and
        recommendations, refer to the <command>mkfs.ocfs2</command> man page.
<!-- Brendo, 17.11.2010: to detect the blocksize of the device, use: 
             blockdev -\-getbsz &lt;block-device&gt;. -->
       </para>
      </entry>
     </row>
<!--https://bugzilla.novell.com/show_bug.cgi?id=586242#c11-->
     <row>
      <entry>
       <para>
        Specific Features On/Off (<option>--fs-features</option>)
       </para>
      </entry>
      <entry>
       <para>
        A comma separated list of feature flags can be provided, and
        <systemitem>mkfs.ocfs2</systemitem> will try to create the file system
        with those features set according to the list. To turn a feature on,
        include it in the list. To turn a feature off, prepend
        <literal>no</literal> to the name.
       </para>
       <para>
        For on overview of all available flags, refer to the
        <command>mkfs.ocfs2</command> man page.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        Pre-Defined Features (<option>--fs-feature-level</option>)
       </para>
      </entry>
      <entry>
       <para>
        Allows you to choose from a set of pre-determined file system features.
        For the available options, refer to the <command>mkfs.ocfs2</command>
        man page.
       </para>
      </entry>
     </row>
    </tbody>
   </tgroup>
  </table>
<!--https://bugzilla.novell.com/show_bug.cgi?id=586242#c15-->
  <para>
   If you do not specify any specific features when creating and formatting the
   volume with <command>mkfs.ocfs2</command>, the following features are
   enabled by default: <option>backup-super</option>, <option>sparse</option>,
   <option>inline-data</option>, <option>unwritten</option>,
   <option>metaecc</option>, <option>indexed-dirs</option>, and
   <option>xattr</option>.
  </para>
<!--<para>The most important flags are:</para>
   <itemizedlist>
   <listitem>
   <para><option>local</option>: Create the file system as a local mount, so
   that it can be mounted without a cluster stack.</para>
   </listitem>
   <listitem>
   <para><option>sparse</option>: Enable support for sparse files. With
   this, OCFS2 can avoid allocating (and zeroing) data to fill holes. Turn
   this feature on if you can, otherwise extends and some writes might be
   less performant.</para>
   </listitem>
   <listitem>
   <para>
   <option>back-super</option>: By default,
   <systemitem>mkfs.ocfs2</systemitem> makes up to 6 backup copies of the
   super block at offsets 1G, 4G, 16G, 64G, 256G and 1T depending on the
   size of the volume. This can be useful in disaster recovery. This
   feature is fully compatible with all versions of the file system and
   generally should not be disabled. </para>
   </listitem>
   <listitem>
   <para><option>unwritten</option>: Enable unwritten extents support. With
   this turned on, an application can request that a range of clusters be
   pre-allocated within a file. OCFS2 will mark those extents with a
   special flag so that expensive data zeroing does not have to be
   performed. Reads and writes to a pre-allocated region act as reads and
   writes to a hole, except a write will not fail due to lack of data
   allocation. This feature requires <option>sparse</option> file support
   to be turned on. </para>
   </listitem>
   <listitem>
   <para><option>extended-slotmap</option>: The slot-map is a hidden file on
   an OCFS2 file system which is used to map mounted nodes to system file
   resources. The extended slot map allows a larger range of possible node
   numbers, which is useful for userspace cluster stacks. This feature is
   automatically turned on when needed.</para>
   </listitem>
   <listitem>
   <para><option>inline-data</option>: Enable inline-data support. If this
   feature is turned on, OCFS2 will store small files and directories
   inside the inode block. Data is transparently moved out to an extent
   when it no longer fits inside the inode block. In some cases, this can
   also make a positive impact on cold cache directory and file operations.
   </para>
   </listitem>
   <listitem>
   <para>
   <option>metaecc</option>: Enables metadata checksums. With this enabled,
   the file system computes and stores the checksums in all metadata
   blocks. It also computes and stores an error correction code capable of
   fixing single bit errors. </para>
   </listitem>
   <listitem>
   <para><option>xattr</option>: Enable extended attributes support. With
   this enabled, users can attach <literal>name:value</literal> pairs to
   objects within the file system. In OCFS2, the names can be up to 255
   bytes in length, terminated by the first NULL byte. While it is not
   required, printable names (ASCII) are recommended. The values can be up
   to 64KB of arbitrary binary data. Attributes can be attached to all
   types of inodes: regular files, directories, symbolic links, device
   nodes, etc. This feature is required for users wanting to use extended
   security facilities like POSIX ACLs or SELinux. </para>
   </listitem>
   
   <listitem>
   <para><option>indexed-dirs</option>: Enable directory indexing support.
   With this feature enabled, the file system creates indexed tree for
   non-inline directory entries. For large scale directories, directory
   entry lookup performance from the indexed tree is faster then from the
   legacy directory blocks.</para>
   </listitem>
   <listitem>
   <para><option>usrquota</option>: Enable user quota support. With this
   feature enabled, the file system will track amount of space and number of
   inodes (files, directories, symbolic links) each user owns. It is then
   possible to limit the maximum amount of space or inodes a user can have.
   See a documentation of the quota-tools package for more details. </para>
   </listitem>
   <listitem>
   <para><option>grpquota</option>: Enable group quota support. With this
   feature enabled, file system will track amount of space and number of
   inodes (files, directories, symbolic links) each group owns. It is then
   possible to limit the maximum amount of space or inodes a group can
   have. See documentation of the quota-tools package for more details.
   </para>
   </listitem>
   <listitem>
   <para><option>refcount</option>: Enables creation of reference counted trees. With this
   enabled, the file system allows users to create inode-based snapshots
   and clones known as reflinks. </para>
   </listitem>-->
  <procedure xml:id="pro-ocfs2-volume">
   <title>Creating and Formatting an OCFS2 Volume</title>
   <para>
    Execute the following steps only on <emphasis>one</emphasis> of the cluster
    nodes.
   </para>
   <step>
    <para>
     Open a terminal window and log in as &rootuser;.
    </para>
   </step>
   <step>
    <para>
     Check if the cluster is online with the command <command>crm
     status</command>.
    </para>
   </step>
   <step>
    <para>
     Create and format the volume using the <command>mkfs.ocfs2</command>
     utility. For information about the syntax for this command, refer to the
     <command>mkfs.ocfs2</command> man page.
    </para>
    <para>
     For example, to create a new OCFS2 file system on
     <filename>/dev/sdb1</filename> that supports up to 32 cluster nodes, enter
     the following commands:
    </para>
<screen>&prompt.root; mkfs.ocfs2 -N 32 /dev/sdb1</screen>
<!-- Brendo, 17.11.2010: maybe add -b 4k in the example (this is common
 for software raids) -->
   </step>
  </procedure>
 </section>
 <section xml:id="sec-ha-ocfs2-mount">
  <title>Mounting OCFS2 Volumes</title>
  <para>
   You can either mount an OCFS2 volume manually or with the cluster manager,
   as described in <xref linkend="pro-ocfs2-mount-cluster"/>.
  </para>
  <procedure xml:id="pro-ocfs2-mount-manual">
   <title>Manually Mounting an OCFS2 Volume</title>
   <step>
    <para>
     Open a terminal window and log in as &rootuser;.
    </para>
   </step>
   <step>
    <para>
     Check if the cluster is online with the command <command>crm
     status</command>.
    </para>
   </step>
   <step>
    <para>
     Mount the volume from the command line, using the <command>mount</command>
     command.
    </para>
   </step>
  </procedure>
  <warning>
   <title>Manually Mounted OCFS2 Devices</title>
   <para>
    If you mount the OCFS2 file system manually for testing purposes, make sure
    to unmount it again before starting to use it by means of &ais;.
   </para>
  </warning>
  <procedure xml:id="pro-ocfs2-mount-cluster">
   <title>Mounting an OCFS2 Volume with the Cluster Manager</title>
   <para>
    To mount an OCFS2 volume with the &ha; software, configure an ocf file
    system resource in the cluster. The following procedure uses the
    <command>crm</command> shell to configure the cluster resources.
    Alternatively, you can also use the &hbgui; to configure the resources.
   </para>
   <step>
    <para>
     Start a shell and log in as &rootuser; or equivalent.
    </para>
   </step>
   <step>
    <para>
     Run <command>crm</command> <option>configure</option>.
    </para>
   </step>
   <step>
    <para>
     Configure Pacemaker to mount the OCFS2 file system on every node in the
     cluster:
    </para>
<screen>&prompt.crm.conf;<command>primitive</command> ocfs2-1 ocf:heartbeat:Filesystem \
      params device="/dev/sdb1" directory="/mnt/shared" fstype="ocfs2" options="acl" \
      op monitor interval="20" timeout="40"</screen>
   </step>
   <step>
    <para>
     Add the file system primitive to the base group you have configured in
     <xref linkend="pro-ocfs2-resources"/>:
    </para>
    <substeps performance="required">
     <step>
      <para>
       Enter
      </para>
<screen>&prompt.crm.conf;edit base-group</screen>
     </step>
<!-- Brendo: again, the group could also be removed -->
     <step>
      <para>
       In the vi editor that opens, modify the group as follows and save your
       changes:
      </para>
<screen>&prompt.crm.conf;group base-group dlm o2cb ocfs2-1</screen>
      <para>
       Due to the base group's internal colocation and ordering, Pacemaker will
       only start the <systemitem class="resource">ocfs2-1</systemitem>
       resource on nodes that also have an o2cb resource already running.
      </para>
     </step>
    </substeps>
   </step>
<!-- Brendo, 17.11.2010: add some more steps in here:
    1.) add a filesystem clone:
        clone ocfs2-1-clone ocfs2-1 meta interleave="true"
    2.) add a colocation for the resources
        colocation ocfs2-stack inf: ocfs2-1-clone o2cb-clone dlm-clone
    3.) add the order constraint:
        order start-ocfs2-stack inf: dlm-clone o2cb-clone ocfs2-1-clone
    Note, that colocation and order most times come with the same resources
    in reversed order -->
   <step>
    <para>
     Review your changes with <command>show</command>. To check if you have
     configured all needed resources, also refer to
     <xref linkend="cha-ha-config-example"/>.
    </para>
   </step>
   <step>
    <para>
     If everything is correct, submit your changes with
     <command>commit</command> and leave the crm live configuration with
     <command>exit</command>.
    </para>
   </step>
  </procedure>
 </section>
 <section xml:id="sec-ha-ocfs2-quota">
  <title>Using Quotas on OCFS2 File Systems</title>
<!--taroth 2011-11-24: fate#310056: the following section is a stub, based on an old RN
   entry,  todo: extend for next revision-->
  <para>
   To use quotas on an OCFS2 file system, create and mount the files system
   with the appropriate quota features or mount options, respectively:
   <literal>ursquota</literal> (quota for individual users) or
   <literal>grpquota</literal> (quota for groups). These features can also be
   enabled later on an unmounted file system using
   <command>tunefs.ocfs2</command>.
  </para>
  <para>
   When a file system has the appropriate quota feature enabled, it tracks in
   its metadata how much space and files each user (or group) uses. Since OCFS2
   treats quota information as file system-internal metadata, you do not need
   to run the <command>quotacheck</command>(8) program. All functionality is
   built into fsck.ocfs2 and the file system driver itself.
  </para>
  <para>
   To enable enforcement of limits imposed on each user or group, run
   <command>quotaon</command>(8) like you would do for any other file system.
  </para>
<!--taroth 2011-11-26: according to jan kara (jack), the limitations for
   repquota and warnquota are now obsolete with solving fate#310056, thus
   removing the following completely-->
<!--<para>
   With OCFS2 file systems, the following commands work as usual:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     quota(1)
    </para>
   </listitem>
   <listitem>
    <para>
     setquota(8)
    </para>
   </listitem>
   <listitem>
    <para>
     edquota(8)
    </para>
   </listitem>
  </itemizedlist>

  <para>
     Because of a limitation in the current Kernel interface, the following
   commands do not work with OCFS2:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     repquota(8)
    </para>
   </listitem>
   <listitem>
    <para>
     warnquota(8)
    </para>
   </listitem>
  </itemizedlist>-->
  <para>
   For performance reasons each cluster node performs quota accounting locally
   and synchronizes this information with a common central storage once per 10
   seconds. This interval is tunable with <command>tunefs.ocfs2</command>,
   options <option>usrquota-sync-interval</option> and
   <option>grpquota-sync-interval</option>. Therefore quota information may not
   be exact at all times and as a consequence users or groups can slightly
   exceed their quota limit when operating on several cluster nodes in
   parallel.
  </para>
 </section>
 <section xml:id="sec-ha-ocfs2-more">
  <title>For More Information</title>
  <para>
   For more information about OCFS2, see the following links:
  </para>
  <variablelist>
   <varlistentry>
    <term><link xlink:href="http://oss.oracle.com/projects/ocfs2/"/></term>
    <listitem>
     <para>
      OCFS2 project home page at Oracle.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><link xlink:href="http://oss.oracle.com/projects/ocfs2/documentation"/></term>
    <listitem>
     <para>
      <citetitle>OCFS2 User's Guide</citetitle>, available from the project
      documentation home page.
     </para>
    </listitem>
   </varlistentry>
  </variablelist><indexterm class="endofrange" startref="idx-filesystems-ocfs2"/>
 </section>
</chapter>
