<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
    type="text/xml"
    title="Profiling step"
?>
<!DOCTYPE chapter
[
   <!ENTITY % entities SYSTEM "entity-decl.ent">
   %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.1" xml:id="cha-ha-requirements">

 <title>System Requirements and Recommendations</title>
 <info>
  <abstract>
   <para>
    The following section informs you about system requirements, and some
    prerequisites for &productnamereg;. It also includes recommendations for
    cluster setup.
   </para>
  </abstract>
 </info>
 <section xml:id="sec-ha-requirements-hw">
  <title>Hardware Requirements</title>
  <para>
   The following list specifies hardware requirements for a cluster based on
   &productnamereg;. These requirements represent the minimum hardware
   configuration. Additional hardware might be necessary, depending on how you
   intend to use your cluster.
  </para>
  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     1 to 32 Linux servers with software as specified in
     <xref linkend="sec-ha-requirements-sw"/>. The servers do not require
     identical hardware (memory, disk space, etc.), but they must have the same
     architecture. Cross-platform clusters are not supported.
    </para>
   </listitem>
   <listitem>
    <para>
     At least two TCP/IP communication media per cluster node. Cluster nodes
     use multicast or unicast for communication so the network equipment must
     support the communication means you want to use. The communication media
     should support a data rate of 100 Mbit/s or higher. Preferably, the
     Ethernet channels should be bonded as described in
     <xref linkend="cha-ha-netbonding"/>. Alternatively, use the second
     interface for a redundant communication channel in &corosync;. See also
     <xref linkend="pro-ha-installation-setup-channel2"/>.
     <remark>bmwiedemann 2015-01-12: are there alternatives to Ethernet? WLAN?
      - taroth 2015-01-12: checked this with the developers, the answer was:
      cable connections are more robust</remark>
    </para>
   </listitem>
   <listitem>
    <para>
     Optional: A shared disk subsystem connected to all servers in the cluster
     from where it needs to be accessed. See
     <xref linkend="sec-ha-requirements-disk"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     A <xref linkend="glo-stonith" xrefstyle="select:title nopage"/> mechanism.
     A &stonith; device is a power switch which the cluster uses to reset nodes
     that are thought to be dead or behaving in a strange manner. This is the
     only reliable way to ensure that no data corruption is performed by nodes
     that hang and only appear to be dead.
    </para>
<!--<important>
     <title>No Support Without &stonith;</title>
     <para>
      A cluster without &stonith; is not supported.
     </para>
    </important>
    <para>
     For more information, refer to <xref linkend="cha-ha-fencing"/>.
     </para>-->
   </listitem>
  </itemizedlist>
 </section>
 <section xml:id="sec-ha-requirements-sw">
  <title>Software Requirements</title>
  <para>
   Ensure that the following software requirements are met:
  </para>
<!--taroth 2011-11-02: for the records, we do *not* support mixed clusters
       (SP1/SP2), exception: during rolling upgrade (info by lmb@ha-devel)-->
  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     &slsreg; &productnumber; (with all available online updates) is installed
     on all nodes that will be part of the cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     &productname; &productnumber; (with all available online updates) is
     installed on all nodes that will be part of the cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     If you want to use &geo; clusters, make sure that &hageo; &productnumber;
     (with all available online updates) is installed on all nodes that will be
     part of the cluster.
    </para>
   </listitem>
  </itemizedlist>
 </section>
 <section xml:id="sec-ha-requirements-disk">
  <title>Storage Requirements</title>
  <para>
   To make data highly available, a shared disk system (Storage Area Network,
   or SAN) is recommended for your cluster. If a shared disk subsystem is used,
   ensure the following:
  </para>
  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     The shared disk system is properly set up and functional according to the
     manufacturer&rsquo;s instructions.
    </para>
   </listitem>
   <listitem>
    <para>
     The disks contained in the shared disk system should be configured to use
     mirroring or RAID to add fault tolerance to the shared disk system.
     Hardware-based RAID is recommended. Host-based software RAID is not
     supported for all configurations.
    </para>
   </listitem>
   <listitem>
    <para>
     If you are using iSCSI for shared disk system access, ensure that you have
     properly configured iSCSI initiators and targets.
    </para>
   </listitem>
   <listitem>
    <para>
     When using DRBD* to implement a mirroring RAID system that distributes
     data across two machines, make sure to only access the device provided by
     DRBD&mdash;never the backing device. Use the same (bonded) NICs that the
     rest of the cluster uses to leverage the redundancy provided there.
    </para>
   </listitem>
  </itemizedlist>
 </section>
 <section xml:id="sec-ha-requirements-other">
  <title>Other Requirements and Recommendations</title>
  <para>
   For a supported and useful &ha; setup, consider the following
   recommendations:
  </para>
  <variablelist>
   <varlistentry xml:id="vle-ha-req-nodes">
    <term>Number of Cluster Nodes</term>
    <listitem>
     <para>
      <remark>taroth 2016-06-26: the following is taken from the Cloud docs - DEVs,
      if this is also true for SLE HA?</remark>
      <remark>krig 2014-08-20: Technically, a cluster can consist of a single node. That seems a
       bit pointless, though. The only reason to have a single-node cluster
       that I can think of would be if the intention is to add more nodes
       later, and you'd want to configure things the "right" way from the
       start. I don't think the recommendation for an odd number of nodes is
       entirely correct since STONITH is required anyway. It's possible to
       run a cluster without quorum as long as STONITH is
       configured. However, this should probably be run by someone with more
       experience than me.</remark>
      Each cluster must consist of at least two cluster nodes.
     </para>
     <important>
      <title>Odd Number of Cluster Nodes</title>
      <para>
       It is strongly recommended to use an <emphasis>odd</emphasis> number of
       cluster nodes with a <emphasis>minimum</emphasis> of three nodes.
      </para>
      <para>
       A cluster needs
       <xref linkend="gloss-quorum" xrefstyle="select:label nopage"/> to keep
       services running. Therefore a three-node cluster can tolerate only
       failure of one node at a time, whereas a five-node cluster can tolerate
       failures of two nodes etc.
      </para>
     </important>
    </listitem>
   </varlistentry>
   <varlistentry xml:id="vle-ha-req-stonith">
    <term>&stonith;</term>
    <listitem>
     <important>
      <title>No Support Without &stonith;</title>
      <para>
       A cluster without &stonith; is not supported.
      </para>
     </important>
     <para>
      For a supported &ha; setup, ensure the following:
     </para>
     <itemizedlist mark="bullet" spacing="normal">
      <listitem>
       <para>
        Each node in the &ha; cluster must have at least one &stonith; device
        (usually a piece of hardware). We strongly recommend multiple &stonith;
        devices per node, unless SBD is used. SBD provides a way to enable
        &stonith; and fencing in clusters without external power switches, but
        it requires shared storage.
       </para>
      </listitem>
      <listitem>
       <para>
        The global cluster options <systemitem>stonith-enabled</systemitem> and
        <systemitem>startup-fencing</systemitem> must be set to
        <literal>true</literal>. As soon as you change them, you will lose
        support.
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
   </varlistentry>
   <varlistentry xml:id="vle-ha-req-communication">
    <term>Redundant Communication Paths</term>
    <listitem>
     <para>
      For a supported &ha; setup, it is required to set up cluster
      communication via two or more redundant paths. This can be done via:
     </para>
     <itemizedlist mark="bullet" spacing="normal">
      <listitem>
       <para>
        <xref linkend="cha-ha-netbonding" xrefstyle="select:title"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        A second communication channel in &corosync;. For details, see
        <xref linkend="pro-ha-installation-setup-channel2"/>.
       </para>
      </listitem>
     </itemizedlist>
     <para>
      If possible, choose network device bonding.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Time Synchronization</term>
    <listitem>
     <para>
      Cluster nodes should synchronize to an NTP server outside the cluster.
      For more information, see the <citetitle>&admin;</citetitle> for &sls;
      &productnumber;, available at
      <link xlink:href="https://documentation.suse.com/"/>. Refer to the chapter
      <citetitle>Time Synchronization with NTP</citetitle>.
     </para>
     <para>
      If nodes are not synchronized, log files and cluster reports are very
      hard to analyze.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>NIC Names</term>
    <listitem>
     <para>
      Must be identical on all nodes.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Host Name and IP Address</term>
    <listitem>
     <para>
      Configure host name resolution by editing the
      <filename>/etc/hosts</filename> file on <emphasis>each</emphasis> server
      in the cluster. To ensure that cluster communication is not slowed down
      or tampered with by any DNS:
     </para>
     <itemizedlist mark="bullet" spacing="normal">
      <listitem>
       <para>
        Use static IP addresses.
       </para>
      </listitem>
      <listitem>
       <para>
        List all cluster nodes in this file with their fully qualified host
        name and short host name. It is essential that members of the cluster
        can find each other by name. If the names are not available, internal
        cluster communication will fail.
       </para>
      </listitem>
     </itemizedlist>
     <para>
      For more information, see the <citetitle>&admin;</citetitle> for &sls;
      &productnumber;, available at
      <link xlink:href="https://documentation.suse.com/"/>. Refer to chapter
      <citetitle>Basic Networking</citetitle>, section <citetitle>Configuring a
      Network Connection with &yast;</citetitle> &gt; <citetitle>Configuring
      Host Name and DNS</citetitle>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry xml:id="vle-ha-req-storage">
    <term>Storage Requirements</term>
    <listitem>
<!-- taroth 2014-04-14: https://bugzilla.novell.com/show_bug.cgi?id=873373 -->
     <para>
      Some services may require shared storage. For requirements, see
      <xref linkend="sec-ha-requirements-disk"/>. You can also use an external
      NFS share or DRBD. If using an external NFS share, it must be reliably
      accessible from all cluster nodes via redundant communication paths. See
      <xref linkend="vle-ha-req-communication"/>.
     </para>
     <para>
      When using SBD as &stonith; device, additional requirements apply for the
      shared storage. For details, see
      <link xlink:href="http://linux-ha.org/wiki/SBD_Fencing"/>, section
      <citetitle>Requirements</citetitle>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry xml:id="vle-ha-req-ssh">
    <term>SSH</term>
    <listitem>
     <para>
      All cluster nodes must be able to access each other via SSH. Tools like
      <command>hb_report</command> or <command>crm_report</command> (for
      troubleshooting) and &hawk;'s <guimenu>History Explorer</guimenu> require
      passwordless SSH access between the nodes, otherwise they can only
      collect data from the current node. In case you use a non-standard SSH
      port, use the <option>-X</option> option (see man page). For example, if
      your SSH port is <literal>3479</literal>, invoke an
      <literal>hb_report</literal> with:
     </para>
<!--taroth 2015-08-22: fate#314906: Support for a non-standard SSH port in hb_reports script-->
<screen>&prompt.root;hb_report -X "-p 3479" [...]</screen>
     <note>
      <title>Regulatory Requirements</title>
      <para>
       If passwordless SSH access does not comply with regulatory requirements,
       you can use the following work-around for <command>hb_report</command>:
      </para>
      <para>
       Create a user that can log in without a password (for example, using
       public key authentication). Configure <command>sudo</command> for this
       user so it does not require a &rootuser; password. Start
       <command>hb_report</command> from command line with the
       <option>-u</option> option to specify the user's name. For more
       information, see the <command>hb_report</command> man page.
      </para>
      <para>
       For the history explorer there is currently no alternative for
       passwordless login.
      </para>
     </note>
    </listitem>
   </varlistentry>
<!--taroth 2011-11-02:  add link to official web page for support status of
    virtual environments as soon as it is available (see thread on ha-devel), 
    info by lmb@ha-devel:
    We support running inside all virtualization environments where SLES is
    fully supported. We have tests covering KVM/Xen/VMWare; but we basically
    simply inherit the support statement from SLES, since SLE HA has no
    specific hardware dependencies.
    -->
  </variablelist>
 </section>
</chapter>
